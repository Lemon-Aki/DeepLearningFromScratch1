{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4-5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhBkzJPPl6CaP/3TZkdXqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lemon-Aki/DeepLearningFromScratch1/blob/main/Chapter4-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hraF92LefMWn"
      },
      "source": [
        "#신경망 학습 절차\n",
        "#학습 : 신경망에는 적응 가능한 매개변수(가중치와 편향)이 있고, 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정\n",
        "#확률적 경사 하강법(Stochastic Gradient Descent, GSD) : 무직위로 선정한 미니배치에 대해 수행하는 경사 하강법\n",
        "#1단계 : 미니배치\n",
        "#훈련 데이터 중 일부 데이터(미니배치)를 무작위로 선정, 이 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
        "#2단계 : 기울기 산출\n",
        "#미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함, 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시\n",
        "#3단계 : 매개변수 갱신\n",
        "#가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
        "#4단계 : 1~3단계 반복 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS7AJ73tkKqP",
        "outputId": "778eb0f8-5f86-4f0b-9812-4e7a77b5b894"
      },
      "source": [
        "#2층 신경망 클래스 구현하기(입력층 -> 은닉층 -> 출력층)\n",
        "# 코랩과 구글드라이드를 연동(인증 필요)\n",
        "#Transport endpoint is not connected 에러시 코랩 재연결\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/MyDrive/DeepLearningFromScratch1/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/DeepLearningFromScratch1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29KA8b7pkrgm",
        "outputId": "ae7a74e1-6332-49aa-c62f-6cf8dd4258ca"
      },
      "source": [
        "import numpy as np\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "  #인스턴스 생성자\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    #가중치 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  #예측 메소드, 테스트 데이터와 행렬 곱연산 후 시그모이드 함수 통과\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = sigmoid(a2)\n",
        "\n",
        "    return y\n",
        "  \n",
        "  #손실 함수 메소드, 훈련 후 교차 엔트로피 오차를 통과(결과가 작은 값인 쪽이 정답일 가능성이 높음)\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  #정확도 메소드, 훈련 결과와 테스트 결과값이 일치한 확률을 표시\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  #경사 하강법 메소드, 가중치 매개변수의 기울기를 구함\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "print(net.params['W1'].shape)\n",
        "print(net.params['b1'].shape)\n",
        "print(net.params['W2'].shape)\n",
        "print(net.params['b2'].shape)\n",
        "\n",
        "x = np.random.rand(100, 784)\n",
        "t = np.random.rand(100, 10)\n",
        "\n",
        "grads = net.numerical_gradient(x, t)\n",
        "\n",
        "print(grads['W1'].shape)\n",
        "print(grads['b1'].shape)\n",
        "print(grads['W2'].shape)\n",
        "print(grads['b2'].shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n",
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "E9FCcJvmq1bK",
        "outputId": "69bd170a-a066-4f70-a196-02265c95eb8d"
      },
      "source": [
        "#미니배치 학습 구현\n",
        "from dataset.mnist import load_mnist\n",
        "(x_train, t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "#1에포크 당 반복 수\n",
        "iters_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "#하이퍼 파라미터\n",
        "iters_num = 10000   #반복 횟수\n",
        "train_size = x_train.shape[0]   #60000개\n",
        "batch_size = 100    #미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  #미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #기울기 계산 - 윗 줄은 시간이 너무 오래걸려서 실행불가\n",
        "  #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  #grad = network.gradient(x_batch, t_batch)    #성능 개선용\n",
        "  \n",
        "  #매개변수갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  #학습경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  #1에포크 당 정확도 계산\n",
        "  if i % iters_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print_x = np.arange(0, iters_num, 1)\n",
        "print_y = train_loss_list\n",
        "plt.plot(print_x, print_y)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.10218333333333333, 0.101\n",
            "train acc, test acc | 0.09035, 0.0892\n",
            "train acc, test acc | 0.09035, 0.0892\n",
            "train acc, test acc | 0.09035, 0.0892\n",
            "train acc, test acc | 0.09035, 0.0892\n",
            "train acc, test acc | 0.09035, 0.0892\n",
            "train acc, test acc | 0.09545, 0.0946\n",
            "train acc, test acc | 0.11215, 0.1134\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.11236666666666667, 0.1135\n",
            "train acc, test acc | 0.10288333333333333, 0.1023\n",
            "train acc, test acc | 0.09871666666666666, 0.098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTQqgsZ3d4hc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}