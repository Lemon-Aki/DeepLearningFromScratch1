{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEB1UxFCRoBDx7cSbiR6YU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lemon-Aki/DeepLearningFromScratch1/blob/main/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvUo02iqEv3"
      },
      "source": [
        "#합성곱 신경망(convolutional neural network,CNN)\n",
        "#합성곱 계층(convolutional layer):\n",
        "#풀링 계층(pooling layer):\n",
        "#완전연결 : 인접한 계층의 모든 뉴런과 연결되어 있는 방식\n",
        "#완전연결계층(Affine 계층)으로 이루어진 연결망 r\n",
        "#입력데이터 -> Affine계층 -> ReLU계층(활성화함수) -> ・・・ -> Affine계층 -> softmax계층(출력층) -> 최종 결과(확률)\n",
        "#CNN 계층 : 출력에 가까운 층에서는 Affine-ReLU 구성이 사용 가능\n",
        "#입력데이터 -> 합성곱계층(Conv) -> ReLU계층(활성화함수) -> 풀링계층 -> ・・・ -> \n",
        "#Affine계층 -> ReLU계층(활성화함수) -> Affine계층 -> softmax계층(출력층) -> 최종 결과(확률)\n",
        "#완전연결 계층의 문제 : 데이터의 형상이 무시(이미지(3차원)를 1차원 데이터로 바꾸는 등)되기 때문에, 3차원에서 갖는 특징, 패턴등 정보가 사라짐\n",
        "#합성곱 계층은 입력으로 받은 데이터의 형상을 그대로 출력함\n",
        "#특징 맵(feature map): 합성곱 계층의 입출력 데이터\n",
        "#합성곱 연산(필터 연산) : 필터의 윈도우를 일정 간격으로 이동해가면 입력데이터에 적용, 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 저장(단일 곱셈-누산,FMA)\n",
        "#패딩(Padding): 입력 데이터 주변을 특정 값으로 채움, 합성곱 연산을 반복하다보면 출력 크기가 입력크기보다 줄어들기때문에, 출력 크기를 조정할 목적으로 사용\n",
        "#스트라이드(stride): 윈도우를 몇칸씩 이동시킬까를 결정\n",
        "#입력크기(H,W), 필터크기(FH,FW), 출력 크기(OH, OW), 패딩(P), 스트라이드(S)일때\n",
        "#출력 크기 : OH = ((H+2P-FH)/s) + 1, OH = ((W+2P-FW)/s) + 1\n",
        "#3차원 합성곱 연산 : 채널이 여러개일 경우, 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더함, 입력 데이터의 채널수 = 필터의 채널수\n",
        "#풀링 계층 : 세로/가로 방향의 공간을 줄이는 연산, 단순 연산이기 때문에 학습해야할 매개변수가 없고, 채널수가 변하지 않음, 입력의 변화에 따른 영향이 적음\n",
        "#최대 풀링 : 지정된 대상 영역에서 최댓값을 구하는 연산"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HKTFlfVqOdq",
        "outputId": "860ff595-0c72-4d07-c88c-4fd1d6a02759"
      },
      "source": [
        "# 코랩과 구글드라이드를 연동(인증 필요)\n",
        "#Transport endpoint is not connected 에러시 코랩 재연결\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/MyDrive/DeepLearningFromScratch1/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/DeepLearningFromScratch1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaBVuMcNqVcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aad6709-531a-4e4d-fe9e-bfb2dc19e1e7"
      },
      "source": [
        "#10개 데이터, 1채널, 28높이, 28너비\n",
        "import numpy as np\n",
        "x = np.random.rand(10, 1, 28, 28)\n",
        "x.shape\n",
        "#x[0,0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmo0H4dqZZrt",
        "outputId": "b7a3111f-c9a6-42db-ec55-c161d85c180a"
      },
      "source": [
        "#im2col함수 : 입력 데이터를 필터링 하기 좋게 전개하는 함수(4차원 데이터를 2차원 행렬로 바꾸는 등)\n",
        "#im2col 단점 : 필터의 적용 영역이 겹치는 경우가 대부분이기 대문에, 필터 적용 영역이 겹치면 전개후 원소 수가 원래 블록 원소 수 보다 많아짐\n",
        "#합성곱 연산의 필터 처리 과정 : im2col에 통과한 입력데이터와 세로 1열로 전개한 필터를 행렬곱연산 후(결과:2차원) reshape(2차원->4차원)\n",
        "#im2col함수 구조 : im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
        "#im2col함수 구조 : im2col(입력데이터(데이터수, 채널수, 높이, 너비의 4차원배열), 필터의 높이, 필터의 너비, 스트라이드, 패딩)\n",
        "from common.util import im2col\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7, 7)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape)\n",
        "\n",
        "x2 = np.random.rand(10, 3, 7, 7)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlIoFe0Fb4ke"
      },
      "source": [
        "#합성곱 계층 구현\n",
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    #필터(가중치), 편향, 스트라이드, 패딩 초기화\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    #reshape의 -1 : 다차원 배열의 원소 수가 변환 후에도 똑같이 유지 되도록 묶어줌\n",
        "    #예 : 10 *3 * 5 *5 =750개 원소를 reshape(10,-1)로 부르면 10묶음, 즉 (10,75)로 묶어줌\n",
        "    col_W = self.W.reshape(FN, -1).T\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "\n",
        "    #transpose함수 : 다차원 배열의 축 순서를 지정한 인덱스에 따라 바꿔줌(아래에선 N, -1(채널수), out_h, out_w 순으로 변경)\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "    self.db = np.sum(dout, axis=0)\n",
        "    self.dW = np.dot(self.col.T, dout)\n",
        "    self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "    dcol = np.dot(dout, self.col_W.T)\n",
        "    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vncMwhBtfrmb"
      },
      "source": [
        "#풀링 계층 구현\n",
        "#입력 데이터를 전개한다 -> 행별 최댓값을 구한다 -> 적절한 모양으로 성형한다\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    #전개\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self,pad)\n",
        "    col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "    \n",
        "    #최댓값구하기\n",
        "    out = np.max(col, axis=1)\n",
        "\n",
        "    #성형\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "    pool_size = self.pool_h * self.pool_w\n",
        "    dmax = np.zeros((dout.size, pool_size))\n",
        "    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "    dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "    return dx"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyghAFT-h9L_"
      },
      "source": [
        "#CNN 구현하기(총3층 : 합성곱계층 -> 완전연결계층(ReLU활성화) -> 완전연결계층(softmax출력))\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "from common import util\n",
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim=(1,28,28), conv_param={'filter_num':30, 'filter_size':5,'pad':0, 'stride':1},\n",
        "               hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "    #합성곱 계층의 하이퍼파라미터 초기화\n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "    #가중치 매개변수 초기화\n",
        "    #W1,b1 : 합성곱계층, W2,b2 : 완전연결계층, W3,b3 : 완전연결계층\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    #zeros : 0으로 가득찬 넘파이 배열 생성\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    #CNN 구성 계층 생성\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['ReLu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.layers['ReLu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "  \n",
        "  #orderdict 계층 순서대로 추론 실행\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  #predict의 결과와 정답 데이터를 마지막 층의 forward에 넘겨줘서 손실함수 값 구하기\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.last_layer.forward(y, t)\n",
        "\n",
        "  #오차역전파법 구현\n",
        "  def gradient(self, x, t):\n",
        "    #순전파\n",
        "    self.loss(x, t)\n",
        "\n",
        "    #역전파\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    #결과저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Conv1'].dW\n",
        "    grads['b1'] = self.layers['Conv1'].db\n",
        "    grads['W2'] = self.layers['Affine1'].dW\n",
        "    grads['b2'] = self.layers['Affine1'].db\n",
        "    grads['W3'] = self.layers['Affine2'].dW\n",
        "    grads['b3'] = self.layers['Affine2'].db\n",
        "\n",
        "    return grads\n",
        "    \n",
        "  def accuracy(self, x, t, batch_size=100):\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "    acc = 0.0\n",
        "        \n",
        "    for i in range(int(x.shape[0] / batch_size)):\n",
        "      tx = x[i*batch_size:(i+1)*batch_size]\n",
        "      tt = t[i*batch_size:(i+1)*batch_size]\n",
        "      y = self.predict(tx)\n",
        "      y = np.argmax(y, axis=1)\n",
        "      acc += np.sum(y == tt) \n",
        "        \n",
        "    return acc / x.shape[0]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dapWtzVMp578",
        "outputId": "2fcfa2aa-ef6d-4dd3-d560-30f7e57adb40"
      },
      "source": [
        "#CNN 훈련 실행\n",
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "#network.save_params(\"params.pkl\")\n",
        "#print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.299839839136071\n",
            "=== epoch:1, train acc:0.255, test acc:0.213 ===\n",
            "train loss:2.2983099719180826\n",
            "train loss:2.292967088388422\n",
            "train loss:2.2906965436514084\n",
            "train loss:2.2784596548664773\n",
            "train loss:2.2667281663372942\n",
            "train loss:2.24827308312369\n",
            "train loss:2.2201315539065973\n",
            "train loss:2.201809187459049\n",
            "train loss:2.172489773745075\n",
            "train loss:2.144926384043697\n",
            "train loss:2.123490451701578\n",
            "train loss:2.1126692458922847\n",
            "train loss:2.0169439422014013\n",
            "train loss:1.964252594673411\n",
            "train loss:1.8988889418535648\n",
            "train loss:1.8467893436473373\n",
            "train loss:1.653386698637396\n",
            "train loss:1.6911475211843803\n",
            "train loss:1.6251487151615394\n",
            "train loss:1.525748423142937\n",
            "train loss:1.3898296280023752\n",
            "train loss:1.3529955761242531\n",
            "train loss:1.1606814830593282\n",
            "train loss:1.1814316803549028\n",
            "train loss:1.1578338834363806\n",
            "train loss:1.069679305031513\n",
            "train loss:0.9499354765286563\n",
            "train loss:0.910148249907629\n",
            "train loss:0.968553512094384\n",
            "train loss:0.8562752229288517\n",
            "train loss:0.8756678756946648\n",
            "train loss:0.7782755411024097\n",
            "train loss:0.9691380176583373\n",
            "train loss:0.8492481949697624\n",
            "train loss:0.672616413337132\n",
            "train loss:0.6893638168562906\n",
            "train loss:0.708729680845048\n",
            "train loss:0.8404715595217749\n",
            "train loss:0.5830015582093823\n",
            "train loss:0.6309126010111203\n",
            "train loss:0.5729815475089243\n",
            "train loss:0.5574089214974334\n",
            "train loss:0.5021789290871367\n",
            "train loss:0.4950248888237328\n",
            "train loss:0.5772123577673206\n",
            "train loss:0.5961137012259544\n",
            "train loss:0.6700128658200184\n",
            "train loss:0.4121727113294348\n",
            "train loss:0.40519415595383074\n",
            "train loss:0.5152327415154654\n",
            "=== epoch:2, train acc:0.818, test acc:0.794 ===\n",
            "train loss:0.6747103602300832\n",
            "train loss:0.5167928237475311\n",
            "train loss:0.56179637932011\n",
            "train loss:0.5837164625368237\n",
            "train loss:0.4040733046927636\n",
            "train loss:0.3366854058988876\n",
            "train loss:0.5469387680707394\n",
            "train loss:0.485189470983584\n",
            "train loss:0.4894519516334151\n",
            "train loss:0.46608440729892336\n",
            "train loss:0.4514566582475237\n",
            "train loss:0.3366687971728255\n",
            "train loss:0.5044752100453668\n",
            "train loss:0.4283117306004814\n",
            "train loss:0.3840546134245559\n",
            "train loss:0.47962788118895633\n",
            "train loss:0.39211126682586395\n",
            "train loss:0.4371245392112837\n",
            "train loss:0.2864971641268871\n",
            "train loss:0.36931943798088507\n",
            "train loss:0.3875852284695292\n",
            "train loss:0.41430387313406447\n",
            "train loss:0.4117762619791223\n",
            "train loss:0.27641091617634433\n",
            "train loss:0.4338912098302648\n",
            "train loss:0.36780267566719277\n",
            "train loss:0.2858250841539876\n",
            "train loss:0.4755329128875993\n",
            "train loss:0.4292563192500174\n",
            "train loss:0.3373738029100862\n",
            "train loss:0.48759381827833237\n",
            "train loss:0.3050775500689155\n",
            "train loss:0.2636581581345153\n",
            "train loss:0.45101755358752194\n",
            "train loss:0.3509564964538016\n",
            "train loss:0.3246402489346717\n",
            "train loss:0.46446722782776634\n",
            "train loss:0.2759136539846384\n",
            "train loss:0.3356172846674765\n",
            "train loss:0.28747179728457484\n",
            "train loss:0.35078460269638223\n",
            "train loss:0.37624647739954176\n",
            "train loss:0.46495710838810667\n",
            "train loss:0.27046218601821564\n",
            "train loss:0.3953234308403285\n",
            "train loss:0.3793913966416617\n",
            "train loss:0.33106971760221504\n",
            "train loss:0.18551200751687033\n",
            "train loss:0.4705907042452964\n",
            "train loss:0.3288506556374189\n",
            "=== epoch:3, train acc:0.876, test acc:0.877 ===\n",
            "train loss:0.30092303412727495\n",
            "train loss:0.2563968107974448\n",
            "train loss:0.3289155071920986\n",
            "train loss:0.34321772818336627\n",
            "train loss:0.48548835593688827\n",
            "train loss:0.30593887333094316\n",
            "train loss:0.34248892114923035\n",
            "train loss:0.4775523794419447\n",
            "train loss:0.28846971238157904\n",
            "train loss:0.38234332129815407\n",
            "train loss:0.6664139960199671\n",
            "train loss:0.3394012537888775\n",
            "train loss:0.2861293923237703\n",
            "train loss:0.41823252543945577\n",
            "train loss:0.349146569445121\n",
            "train loss:0.34658075900510726\n",
            "train loss:0.4224613491818735\n",
            "train loss:0.3635344476062633\n",
            "train loss:0.1843737516526945\n",
            "train loss:0.39132216722692165\n",
            "train loss:0.3906321222447077\n",
            "train loss:0.29108633973544984\n",
            "train loss:0.26125482964923963\n",
            "train loss:0.3074707040874577\n",
            "train loss:0.41739484644982966\n",
            "train loss:0.2778501261419813\n",
            "train loss:0.3194150492774262\n",
            "train loss:0.2887034114929891\n",
            "train loss:0.4058463494106484\n",
            "train loss:0.35677685282143284\n",
            "train loss:0.5476570180549486\n",
            "train loss:0.3560134819375206\n",
            "train loss:0.5695829734829225\n",
            "train loss:0.2982311520163656\n",
            "train loss:0.29552240519472167\n",
            "train loss:0.369281651408091\n",
            "train loss:0.28996706715718656\n",
            "train loss:0.28020091147867865\n",
            "train loss:0.3318740334407344\n",
            "train loss:0.18142800151294317\n",
            "train loss:0.31042572857792755\n",
            "train loss:0.24211779957026203\n",
            "train loss:0.24118164219568705\n",
            "train loss:0.28954868835297526\n",
            "train loss:0.29594218187829\n",
            "train loss:0.1981186271808519\n",
            "train loss:0.235845478168354\n",
            "train loss:0.2035464848151103\n",
            "train loss:0.18717934203298192\n",
            "train loss:0.21935036291389498\n",
            "=== epoch:4, train acc:0.885, test acc:0.887 ===\n",
            "train loss:0.35680515989434697\n",
            "train loss:0.31288519842763746\n",
            "train loss:0.4736811443219008\n",
            "train loss:0.24173116355634072\n",
            "train loss:0.16357038599497561\n",
            "train loss:0.2579192574847839\n",
            "train loss:0.48495117420607153\n",
            "train loss:0.1740057001468391\n",
            "train loss:0.3419205125240798\n",
            "train loss:0.1665712620561077\n",
            "train loss:0.3291737202747611\n",
            "train loss:0.21931312775522394\n",
            "train loss:0.30625953802860745\n",
            "train loss:0.29794610192812987\n",
            "train loss:0.49137105467030834\n",
            "train loss:0.20995866408855288\n",
            "train loss:0.1420843343480693\n",
            "train loss:0.18507403096344016\n",
            "train loss:0.24703570109469516\n",
            "train loss:0.23356565507850388\n",
            "train loss:0.14909793121731008\n",
            "train loss:0.38193956589126776\n",
            "train loss:0.2772386309360109\n",
            "train loss:0.2448524936014321\n",
            "train loss:0.19458781582340975\n",
            "train loss:0.23161130244280898\n",
            "train loss:0.2790744738871654\n",
            "train loss:0.19281474178958294\n",
            "train loss:0.3150424480329707\n",
            "train loss:0.13328917621034772\n",
            "train loss:0.1466291225358145\n",
            "train loss:0.41081742948560257\n",
            "train loss:0.1379683191857954\n",
            "train loss:0.343923807320335\n",
            "train loss:0.3319493016515785\n",
            "train loss:0.2201961700624198\n",
            "train loss:0.22595195313376443\n",
            "train loss:0.28956586638121606\n",
            "train loss:0.18852590310499914\n",
            "train loss:0.11865693702297028\n",
            "train loss:0.22190606358613418\n",
            "train loss:0.215871928189582\n",
            "train loss:0.19067542838179083\n",
            "train loss:0.38412359036969457\n",
            "train loss:0.17481939609201394\n",
            "train loss:0.2652833338142928\n",
            "train loss:0.2387215797216115\n",
            "train loss:0.2564243022686936\n",
            "train loss:0.39677374094882006\n",
            "train loss:0.1402419883122964\n",
            "=== epoch:5, train acc:0.905, test acc:0.9 ===\n",
            "train loss:0.23543523653966286\n",
            "train loss:0.2589992663863484\n",
            "train loss:0.2766576139717479\n",
            "train loss:0.1448209301347432\n",
            "train loss:0.3420098239097177\n",
            "train loss:0.18574471564251663\n",
            "train loss:0.2929028805633631\n",
            "train loss:0.20697559038825944\n",
            "train loss:0.2832189116107964\n",
            "train loss:0.2536537549413259\n",
            "train loss:0.19497854431094028\n",
            "train loss:0.11260120480445218\n",
            "train loss:0.1891397315483724\n",
            "train loss:0.15133814326790365\n",
            "train loss:0.1790275843702758\n",
            "train loss:0.2722729722206032\n",
            "train loss:0.2538329792323583\n",
            "train loss:0.32666370992342947\n",
            "train loss:0.1976119365126562\n",
            "train loss:0.42554053872193875\n",
            "train loss:0.28225110589241814\n",
            "train loss:0.3576812457486875\n",
            "train loss:0.15773365866103548\n",
            "train loss:0.19479748218212464\n",
            "train loss:0.24985737949135053\n",
            "train loss:0.1627777468320154\n",
            "train loss:0.16236343652671026\n",
            "train loss:0.3426183496333001\n",
            "train loss:0.25021218673721046\n",
            "train loss:0.30345216781559503\n",
            "train loss:0.2061469978157333\n",
            "train loss:0.26303605357818594\n",
            "train loss:0.15543626515835074\n",
            "train loss:0.1682896769374965\n",
            "train loss:0.1793328457442509\n",
            "train loss:0.12790521439356536\n",
            "train loss:0.150180096025978\n",
            "train loss:0.16448834608204618\n",
            "train loss:0.3877186404901682\n",
            "train loss:0.2060768928927471\n",
            "train loss:0.2610418494388045\n",
            "train loss:0.22099365832999252\n",
            "train loss:0.2209931728906034\n",
            "train loss:0.28043952236653946\n",
            "train loss:0.12425837635972994\n",
            "train loss:0.14450097393754746\n",
            "train loss:0.2183035675522621\n",
            "train loss:0.2067502902750254\n",
            "train loss:0.3083069937682501\n",
            "train loss:0.12095466465531579\n",
            "=== epoch:6, train acc:0.919, test acc:0.9 ===\n",
            "train loss:0.22468041630009694\n",
            "train loss:0.11299850012074161\n",
            "train loss:0.1266916206669112\n",
            "train loss:0.14499699605125918\n",
            "train loss:0.28708617547798243\n",
            "train loss:0.21307379338862462\n",
            "train loss:0.21673127282885363\n",
            "train loss:0.15662128937092046\n",
            "train loss:0.27660772771763203\n",
            "train loss:0.10010895405665585\n",
            "train loss:0.10845803536613557\n",
            "train loss:0.23841282208464387\n",
            "train loss:0.1183311523518951\n",
            "train loss:0.23317861111769755\n",
            "train loss:0.20445205859538707\n",
            "train loss:0.22979501536433866\n",
            "train loss:0.19140347262028\n",
            "train loss:0.16073182937992084\n",
            "train loss:0.15423145783785294\n",
            "train loss:0.27855301103104235\n",
            "train loss:0.13921262672964355\n",
            "train loss:0.16516828240839543\n",
            "train loss:0.10195336249837476\n",
            "train loss:0.2639536608807565\n",
            "train loss:0.21540691315844776\n",
            "train loss:0.18367560283108028\n",
            "train loss:0.15809709718297535\n",
            "train loss:0.2782592656147153\n",
            "train loss:0.12759309425469254\n",
            "train loss:0.17045309376643059\n",
            "train loss:0.17211697909974916\n",
            "train loss:0.1160102895792462\n",
            "train loss:0.09655443982018223\n",
            "train loss:0.08604695753134736\n",
            "train loss:0.15856309262043208\n",
            "train loss:0.11172372618032061\n",
            "train loss:0.0741113973478614\n",
            "train loss:0.31824592893394\n",
            "train loss:0.14381485847749753\n",
            "train loss:0.19336404757314354\n",
            "train loss:0.10732579239990442\n",
            "train loss:0.17894434625904396\n",
            "train loss:0.19367840984330673\n",
            "train loss:0.1972658260585714\n",
            "train loss:0.17957324333707145\n",
            "train loss:0.13198646018028848\n",
            "train loss:0.14608424025920486\n",
            "train loss:0.2467067078944027\n",
            "train loss:0.08731490895749466\n",
            "train loss:0.17863428303486706\n",
            "=== epoch:7, train acc:0.944, test acc:0.921 ===\n",
            "train loss:0.1214133820809892\n",
            "train loss:0.2050518380092397\n",
            "train loss:0.11261014588978074\n",
            "train loss:0.20099115673508602\n",
            "train loss:0.07908819641230941\n",
            "train loss:0.17653030250606416\n",
            "train loss:0.10546808025595183\n",
            "train loss:0.1437901381194697\n",
            "train loss:0.2156505790528666\n",
            "train loss:0.12283960051148143\n",
            "train loss:0.1566310889230243\n",
            "train loss:0.12585374353586798\n",
            "train loss:0.23126268081967857\n",
            "train loss:0.07936418156627735\n",
            "train loss:0.1355448372628427\n",
            "train loss:0.20156059031883075\n",
            "train loss:0.23193009548383994\n",
            "train loss:0.17598045305622784\n",
            "train loss:0.15801021195482232\n",
            "train loss:0.10732402925678496\n",
            "train loss:0.09121155109883938\n",
            "train loss:0.17727734179591195\n",
            "train loss:0.1799362060129849\n",
            "train loss:0.09912737322490184\n",
            "train loss:0.12225566113241478\n",
            "train loss:0.22261198371350419\n",
            "train loss:0.12412885268432133\n",
            "train loss:0.31484813069422024\n",
            "train loss:0.11538400710082312\n",
            "train loss:0.14607964158536169\n",
            "train loss:0.04123409569549251\n",
            "train loss:0.11110006688553398\n",
            "train loss:0.20221088984410976\n",
            "train loss:0.17005178953024902\n",
            "train loss:0.18385738032505977\n",
            "train loss:0.15673974435624144\n",
            "train loss:0.15373897194044941\n",
            "train loss:0.22958749504620374\n",
            "train loss:0.2769355476450377\n",
            "train loss:0.10701801514858886\n",
            "train loss:0.17910869248084943\n",
            "train loss:0.1594744612757845\n",
            "train loss:0.12177883119095037\n",
            "train loss:0.23805266260623903\n",
            "train loss:0.15408424070446258\n",
            "train loss:0.24340471035637815\n",
            "train loss:0.19535749779922879\n",
            "train loss:0.15405382497504483\n",
            "train loss:0.13413141659877792\n",
            "train loss:0.06714193396959829\n",
            "=== epoch:8, train acc:0.931, test acc:0.929 ===\n",
            "train loss:0.13561396052816563\n",
            "train loss:0.07259084275289837\n",
            "train loss:0.1267393241710885\n",
            "train loss:0.15443053392618994\n",
            "train loss:0.14899531876307268\n",
            "train loss:0.16335671517622852\n",
            "train loss:0.19603009701877752\n",
            "train loss:0.12140281656299154\n",
            "train loss:0.1960888959267941\n",
            "train loss:0.0834482785863918\n",
            "train loss:0.252863001239105\n",
            "train loss:0.22767119127890809\n",
            "train loss:0.19926430265179396\n",
            "train loss:0.1300882163157596\n",
            "train loss:0.1094417165936985\n",
            "train loss:0.11768695388300694\n",
            "train loss:0.22346303640386045\n",
            "train loss:0.10554973940088437\n",
            "train loss:0.09655800601025095\n",
            "train loss:0.1367752973965909\n",
            "train loss:0.13424420181005786\n",
            "train loss:0.11359455205901224\n",
            "train loss:0.15593491171250895\n",
            "train loss:0.15007850142821094\n",
            "train loss:0.13388691828679922\n",
            "train loss:0.07032814004701858\n",
            "train loss:0.14968442813020308\n",
            "train loss:0.14864887375902008\n",
            "train loss:0.12746888314668178\n",
            "train loss:0.10442830896230092\n",
            "train loss:0.08564038279626848\n",
            "train loss:0.07260611707069844\n",
            "train loss:0.28415257707522923\n",
            "train loss:0.1357756879220159\n",
            "train loss:0.15158363652358167\n",
            "train loss:0.1140568878735313\n",
            "train loss:0.12634842072226823\n",
            "train loss:0.10599607142966358\n",
            "train loss:0.047740500451791845\n",
            "train loss:0.1528771752827974\n",
            "train loss:0.15756764943481508\n",
            "train loss:0.23802206222769876\n",
            "train loss:0.15629105264553242\n",
            "train loss:0.060433519640259975\n",
            "train loss:0.1402670914719976\n",
            "train loss:0.16027085668177674\n",
            "train loss:0.1788256578357469\n",
            "train loss:0.1031591393643949\n",
            "train loss:0.23015017795934803\n",
            "train loss:0.12095378665978174\n",
            "=== epoch:9, train acc:0.94, test acc:0.922 ===\n",
            "train loss:0.0754106504883261\n",
            "train loss:0.18315400079942976\n",
            "train loss:0.1461560111288073\n",
            "train loss:0.11120169745496339\n",
            "train loss:0.08483739952703484\n",
            "train loss:0.07277778823119714\n",
            "train loss:0.1337486974956311\n",
            "train loss:0.0580212061753698\n",
            "train loss:0.16382296614313813\n",
            "train loss:0.11913931284230886\n",
            "train loss:0.19571042799855867\n",
            "train loss:0.09087235109414943\n",
            "train loss:0.12399312701828867\n",
            "train loss:0.1099479573508574\n",
            "train loss:0.181729881922071\n",
            "train loss:0.09986394147595788\n",
            "train loss:0.16520500532878596\n",
            "train loss:0.10081838475487351\n",
            "train loss:0.06655449426669546\n",
            "train loss:0.08982614333757624\n",
            "train loss:0.14132133484538825\n",
            "train loss:0.12352451353593569\n",
            "train loss:0.15591576385011602\n",
            "train loss:0.12283451195805054\n",
            "train loss:0.21037634317313372\n",
            "train loss:0.11790008508888981\n",
            "train loss:0.044085111797306106\n",
            "train loss:0.16093137396054033\n",
            "train loss:0.11871156854621036\n",
            "train loss:0.10790986983863995\n",
            "train loss:0.10140188979045173\n",
            "train loss:0.11192869346146464\n",
            "train loss:0.07506874734740686\n",
            "train loss:0.08888243286318863\n",
            "train loss:0.049193051604830194\n",
            "train loss:0.08950808303020633\n",
            "train loss:0.05719103363116955\n",
            "train loss:0.07844030979346182\n",
            "train loss:0.10083908631665593\n",
            "train loss:0.21986536686267047\n",
            "train loss:0.10249318370890981\n",
            "train loss:0.06624687387936919\n",
            "train loss:0.13202416003423353\n",
            "train loss:0.08015907134843508\n",
            "train loss:0.08032864567973094\n",
            "train loss:0.09639331808090905\n",
            "train loss:0.08272321078739013\n",
            "train loss:0.09871759315654818\n",
            "train loss:0.09446128410195324\n",
            "train loss:0.11157210779823033\n",
            "=== epoch:10, train acc:0.96, test acc:0.934 ===\n",
            "train loss:0.05457468012334393\n",
            "train loss:0.05658677827938795\n",
            "train loss:0.09608662878337636\n",
            "train loss:0.060522245566732226\n",
            "train loss:0.09657214457276263\n",
            "train loss:0.05017968615930614\n",
            "train loss:0.06201544066436286\n",
            "train loss:0.06447171838658163\n",
            "train loss:0.09591130594601469\n",
            "train loss:0.0842625415243729\n",
            "train loss:0.1302568653048816\n",
            "train loss:0.19683926449074587\n",
            "train loss:0.09980937061479311\n",
            "train loss:0.06763936863799783\n",
            "train loss:0.09354180333842406\n",
            "train loss:0.06304325002033785\n",
            "train loss:0.10711675468518886\n",
            "train loss:0.10236247307449425\n",
            "train loss:0.24044909897984698\n",
            "train loss:0.02522036746682156\n",
            "train loss:0.06949306364648124\n",
            "train loss:0.0380515982103215\n",
            "train loss:0.1379647094771771\n",
            "train loss:0.20254697710954297\n",
            "train loss:0.09333957358578893\n",
            "train loss:0.08767196265960096\n",
            "train loss:0.07437603013492725\n",
            "train loss:0.18649590041400227\n",
            "train loss:0.2450350986699651\n",
            "train loss:0.170915826903203\n",
            "train loss:0.05479606087808417\n",
            "train loss:0.09629492746314566\n",
            "train loss:0.14975275936902063\n",
            "train loss:0.07297272759073369\n",
            "train loss:0.12638522179414877\n",
            "train loss:0.11936111745200362\n",
            "train loss:0.1382777370171902\n",
            "train loss:0.08829923335874355\n",
            "train loss:0.1663282546914183\n",
            "train loss:0.036488152854347984\n",
            "train loss:0.08022227614368292\n",
            "train loss:0.10638523165134521\n",
            "train loss:0.07495914876327317\n",
            "train loss:0.08080090511585841\n",
            "train loss:0.15372012613707764\n",
            "train loss:0.16948642930524144\n",
            "train loss:0.08033656580798328\n",
            "train loss:0.12526626744957814\n",
            "train loss:0.16302678588830546\n",
            "train loss:0.12243183550213939\n",
            "=== epoch:11, train acc:0.959, test acc:0.929 ===\n",
            "train loss:0.21257798266675185\n",
            "train loss:0.06129579276108756\n",
            "train loss:0.07851978880256788\n",
            "train loss:0.11493600815057954\n",
            "train loss:0.07880062425468123\n",
            "train loss:0.12929590408080993\n",
            "train loss:0.059002546777401445\n",
            "train loss:0.13966225273730978\n",
            "train loss:0.11651924741434007\n",
            "train loss:0.0653467538182999\n",
            "train loss:0.10659820029690709\n",
            "train loss:0.08236014213729709\n",
            "train loss:0.10348638304211785\n",
            "train loss:0.11038592621421797\n",
            "train loss:0.08440779709296041\n",
            "train loss:0.0478781414833942\n",
            "train loss:0.06774735050107211\n",
            "train loss:0.13405474470462486\n",
            "train loss:0.1278845832079721\n",
            "train loss:0.051914094892387554\n",
            "train loss:0.10436892360006884\n",
            "train loss:0.10120269798519145\n",
            "train loss:0.0516632372854932\n",
            "train loss:0.13508391938529968\n",
            "train loss:0.1054502887738862\n",
            "train loss:0.1963555352850348\n",
            "train loss:0.10732603643216987\n",
            "train loss:0.045719253243592314\n",
            "train loss:0.11424495694983638\n",
            "train loss:0.06724976394657783\n",
            "train loss:0.09423053269010438\n",
            "train loss:0.14861221969783264\n",
            "train loss:0.034581702409917196\n",
            "train loss:0.1673109498800518\n",
            "train loss:0.12778484715878782\n",
            "train loss:0.1275875343065528\n",
            "train loss:0.17452050544379671\n",
            "train loss:0.07877756542674262\n",
            "train loss:0.07385081922454123\n",
            "train loss:0.03493061581819299\n",
            "train loss:0.06399433671179001\n",
            "train loss:0.03263758437512452\n",
            "train loss:0.09738894024308932\n",
            "train loss:0.04842429598200965\n",
            "train loss:0.08823691001228744\n",
            "train loss:0.10407121913047426\n",
            "train loss:0.08787708281601045\n",
            "train loss:0.07269100954672973\n",
            "train loss:0.163393848463172\n",
            "train loss:0.11062649676241515\n",
            "=== epoch:12, train acc:0.967, test acc:0.941 ===\n",
            "train loss:0.1644318560855749\n",
            "train loss:0.11913345179230003\n",
            "train loss:0.07781908404431138\n",
            "train loss:0.09673704198693678\n",
            "train loss:0.050807633879579865\n",
            "train loss:0.04153495362486441\n",
            "train loss:0.15395715405036145\n",
            "train loss:0.06277557819856776\n",
            "train loss:0.12329304018597168\n",
            "train loss:0.07770904909964496\n",
            "train loss:0.051345919040431454\n",
            "train loss:0.06933991306163236\n",
            "train loss:0.05958275346223348\n",
            "train loss:0.05477377009127239\n",
            "train loss:0.05004821265265433\n",
            "train loss:0.08791672322089078\n",
            "train loss:0.03448095622457131\n",
            "train loss:0.06976127728507678\n",
            "train loss:0.030160812987330296\n",
            "train loss:0.05627830326732155\n",
            "train loss:0.07549800579330547\n",
            "train loss:0.06327885349655753\n",
            "train loss:0.05163330769558934\n",
            "train loss:0.03146798809450316\n",
            "train loss:0.166755926984276\n",
            "train loss:0.06864476404406311\n",
            "train loss:0.062194566015438595\n",
            "train loss:0.1846617913448922\n",
            "train loss:0.06931968235803096\n",
            "train loss:0.06243154316688978\n",
            "train loss:0.09663662333611328\n",
            "train loss:0.02812197635076164\n",
            "train loss:0.026459204902015337\n",
            "train loss:0.0421295054268777\n",
            "train loss:0.03690211438976143\n",
            "train loss:0.08004661724032738\n",
            "train loss:0.10370393266555004\n",
            "train loss:0.17425701621999845\n",
            "train loss:0.17481331876054082\n",
            "train loss:0.02651517712673121\n",
            "train loss:0.09039985509626756\n",
            "train loss:0.13219856154345075\n",
            "train loss:0.0619570087294886\n",
            "train loss:0.023325232583602834\n",
            "train loss:0.12105077561405553\n",
            "train loss:0.03657886200521536\n",
            "train loss:0.07339833950387967\n",
            "train loss:0.09751251117529308\n",
            "train loss:0.1299084146291401\n",
            "train loss:0.07477594490467686\n",
            "=== epoch:13, train acc:0.976, test acc:0.952 ===\n",
            "train loss:0.07379168772937081\n",
            "train loss:0.041837068905270176\n",
            "train loss:0.06389034048004102\n",
            "train loss:0.09986514157022158\n",
            "train loss:0.07109189524577322\n",
            "train loss:0.15632642535510224\n",
            "train loss:0.04988383430430351\n",
            "train loss:0.07559862429481205\n",
            "train loss:0.09279813016153055\n",
            "train loss:0.07580745721142446\n",
            "train loss:0.08448299226674189\n",
            "train loss:0.0767155775104713\n",
            "train loss:0.12318577323804847\n",
            "train loss:0.09601686416290407\n",
            "train loss:0.11487918504886702\n",
            "train loss:0.05151945006682904\n",
            "train loss:0.033136754024778704\n",
            "train loss:0.08207803690976082\n",
            "train loss:0.06904503279398952\n",
            "train loss:0.04587790732473547\n",
            "train loss:0.03975616744595399\n",
            "train loss:0.04599155674798249\n",
            "train loss:0.04845260846223067\n",
            "train loss:0.03856130181179673\n",
            "train loss:0.0832074390828681\n",
            "train loss:0.05641023062750306\n",
            "train loss:0.032409700724916626\n",
            "train loss:0.033941823183342\n",
            "train loss:0.03916011570069114\n",
            "train loss:0.05238765247347346\n",
            "train loss:0.060063625424901984\n",
            "train loss:0.0505388849071494\n",
            "train loss:0.03495028004759709\n",
            "train loss:0.06331174500887272\n",
            "train loss:0.05432451197582751\n",
            "train loss:0.03356482820835214\n",
            "train loss:0.032536041836235194\n",
            "train loss:0.06480010744665829\n",
            "train loss:0.03930218119259504\n",
            "train loss:0.11241833235783372\n",
            "train loss:0.09382352601219889\n",
            "train loss:0.05534652435296126\n",
            "train loss:0.04754227927145557\n",
            "train loss:0.02288629566139947\n",
            "train loss:0.09436579610952191\n",
            "train loss:0.07899048716742739\n",
            "train loss:0.09811810552094881\n",
            "train loss:0.17937765597863012\n",
            "train loss:0.1610928716903864\n",
            "train loss:0.03966980377819812\n",
            "=== epoch:14, train acc:0.971, test acc:0.947 ===\n",
            "train loss:0.022888742508506324\n",
            "train loss:0.10164196952292652\n",
            "train loss:0.03585487637266744\n",
            "train loss:0.062076318781590784\n",
            "train loss:0.070530823723594\n",
            "train loss:0.08447713439011477\n",
            "train loss:0.05330061351806115\n",
            "train loss:0.07279214076617901\n",
            "train loss:0.054126331072291325\n",
            "train loss:0.13379703836257523\n",
            "train loss:0.03535660336716654\n",
            "train loss:0.036131967427770556\n",
            "train loss:0.05371452280512448\n",
            "train loss:0.03533820959991438\n",
            "train loss:0.14703440103848245\n",
            "train loss:0.06318033800482896\n",
            "train loss:0.02551528996682121\n",
            "train loss:0.06053871905840034\n",
            "train loss:0.030425324100329675\n",
            "train loss:0.043131909244648846\n",
            "train loss:0.04796232680817854\n",
            "train loss:0.05379338375184992\n",
            "train loss:0.05208223816194026\n",
            "train loss:0.04181649706923772\n",
            "train loss:0.03040959910269891\n",
            "train loss:0.02854348922269274\n",
            "train loss:0.04519407258516817\n",
            "train loss:0.0619986141327519\n",
            "train loss:0.08371650779258503\n",
            "train loss:0.10965516494319451\n",
            "train loss:0.04534371875072218\n",
            "train loss:0.06136158568065576\n",
            "train loss:0.062485469269256015\n",
            "train loss:0.015669110245622925\n",
            "train loss:0.09142327714221558\n",
            "train loss:0.1435285869553102\n",
            "train loss:0.08018443521594046\n",
            "train loss:0.02838785524711328\n",
            "train loss:0.09866062328490999\n",
            "train loss:0.024789454168188847\n",
            "train loss:0.07945869122274148\n",
            "train loss:0.03560472690052025\n",
            "train loss:0.08807380531350026\n",
            "train loss:0.03774396741806855\n",
            "train loss:0.032932148658514786\n",
            "train loss:0.057451242498664186\n",
            "train loss:0.06852333082207433\n",
            "train loss:0.021062948788522724\n",
            "train loss:0.042263658048210584\n",
            "train loss:0.03580547082909366\n",
            "=== epoch:15, train acc:0.98, test acc:0.942 ===\n",
            "train loss:0.09739902258963604\n",
            "train loss:0.0902235614212563\n",
            "train loss:0.029254036152081\n",
            "train loss:0.05426504383092456\n",
            "train loss:0.04506125082031391\n",
            "train loss:0.04186675449782602\n",
            "train loss:0.06569091836035622\n",
            "train loss:0.04563936068217819\n",
            "train loss:0.02684191869742123\n",
            "train loss:0.02304368834518245\n",
            "train loss:0.10213515427468564\n",
            "train loss:0.05693490456678402\n",
            "train loss:0.03523369802843012\n",
            "train loss:0.020171985268906765\n",
            "train loss:0.07057544587413611\n",
            "train loss:0.029105147446304307\n",
            "train loss:0.058246907880732265\n",
            "train loss:0.04431794829941901\n",
            "train loss:0.034766017111097884\n",
            "train loss:0.09215889171514506\n",
            "train loss:0.06871758555151011\n",
            "train loss:0.02861084232314864\n",
            "train loss:0.0768557211538306\n",
            "train loss:0.024430340126447958\n",
            "train loss:0.039554535669649366\n",
            "train loss:0.04078198787474792\n",
            "train loss:0.08286375734158664\n",
            "train loss:0.044359844718743574\n",
            "train loss:0.04613076440724748\n",
            "train loss:0.05682341673713867\n",
            "train loss:0.051915619276883096\n",
            "train loss:0.027887837284770223\n",
            "train loss:0.04678514904367411\n",
            "train loss:0.06625513529806762\n",
            "train loss:0.027083943357163118\n",
            "train loss:0.04011242458454999\n",
            "train loss:0.026663573112000655\n",
            "train loss:0.04275003091870231\n",
            "train loss:0.02694191388555722\n",
            "train loss:0.015998711231111643\n",
            "train loss:0.030243666119751916\n",
            "train loss:0.04033934197183121\n",
            "train loss:0.06036549390140689\n",
            "train loss:0.01912979450889043\n",
            "train loss:0.02802068490498788\n",
            "train loss:0.02862820875598428\n",
            "train loss:0.03924157371108907\n",
            "train loss:0.03972320364507453\n",
            "train loss:0.012765401263979823\n",
            "train loss:0.04363898006594596\n",
            "=== epoch:16, train acc:0.985, test acc:0.95 ===\n",
            "train loss:0.07027294705680398\n",
            "train loss:0.015791453264553685\n",
            "train loss:0.04070685947713763\n",
            "train loss:0.07700335950697638\n",
            "train loss:0.030432074141356732\n",
            "train loss:0.07415779796325211\n",
            "train loss:0.07837240485497529\n",
            "train loss:0.050695162968906304\n",
            "train loss:0.0214248264993472\n",
            "train loss:0.0343095445969169\n",
            "train loss:0.04129859960972226\n",
            "train loss:0.08356961815446388\n",
            "train loss:0.0583672321035684\n",
            "train loss:0.03902862425427171\n",
            "train loss:0.02876016191029477\n",
            "train loss:0.020719935392837886\n",
            "train loss:0.032554224229667056\n",
            "train loss:0.023747305104142757\n",
            "train loss:0.07091374841763282\n",
            "train loss:0.059644899494014136\n",
            "train loss:0.02183571547735815\n",
            "train loss:0.10240347118256937\n",
            "train loss:0.017689721211167814\n",
            "train loss:0.05113317556566192\n",
            "train loss:0.022171479279728065\n",
            "train loss:0.055487962536162845\n",
            "train loss:0.06501837134296723\n",
            "train loss:0.04187821730903858\n",
            "train loss:0.022817268523171917\n",
            "train loss:0.027721189421344702\n",
            "train loss:0.02997265153949502\n",
            "train loss:0.02057820653439761\n",
            "train loss:0.05370856998344954\n",
            "train loss:0.03483628343089462\n",
            "train loss:0.03106291147532205\n",
            "train loss:0.027326259321684455\n",
            "train loss:0.028910971971491867\n",
            "train loss:0.06442174906493704\n",
            "train loss:0.012639004627711553\n",
            "train loss:0.05923275864586015\n",
            "train loss:0.060062816285110364\n",
            "train loss:0.019584512599924914\n",
            "train loss:0.017070860534685407\n",
            "train loss:0.03699997806369415\n",
            "train loss:0.03899855277782372\n",
            "train loss:0.05046324691901745\n",
            "train loss:0.04818738621670814\n",
            "train loss:0.039556292458739054\n",
            "train loss:0.05194229556001902\n",
            "train loss:0.039506122372336326\n",
            "=== epoch:17, train acc:0.984, test acc:0.952 ===\n",
            "train loss:0.05445782885382307\n",
            "train loss:0.016736354476912567\n",
            "train loss:0.03562625077349557\n",
            "train loss:0.013251271202316508\n",
            "train loss:0.018572900184270495\n",
            "train loss:0.07479234920152372\n",
            "train loss:0.07225799237081543\n",
            "train loss:0.015136592593837069\n",
            "train loss:0.017980988620705622\n",
            "train loss:0.013906087335061483\n",
            "train loss:0.1304254466659476\n",
            "train loss:0.04989018170385228\n",
            "train loss:0.026132223995268232\n",
            "train loss:0.0260724546204807\n",
            "train loss:0.03945067472381128\n",
            "train loss:0.014516381452824538\n",
            "train loss:0.02472572160586971\n",
            "train loss:0.024731947461942284\n",
            "train loss:0.02966685350644677\n",
            "train loss:0.010249543858623464\n",
            "train loss:0.043763967158851926\n",
            "train loss:0.016917221768002443\n",
            "train loss:0.03291416415895981\n",
            "train loss:0.014437450096424624\n",
            "train loss:0.02603203182126948\n",
            "train loss:0.019370136236255324\n",
            "train loss:0.05563389931323447\n",
            "train loss:0.030849697872489514\n",
            "train loss:0.0325506866486154\n",
            "train loss:0.025079074592996062\n",
            "train loss:0.0247166500549969\n",
            "train loss:0.019074122097984883\n",
            "train loss:0.027036503229654108\n",
            "train loss:0.017432403462416948\n",
            "train loss:0.0321008981141001\n",
            "train loss:0.03379143905741964\n",
            "train loss:0.015026586644648622\n",
            "train loss:0.061379319322268956\n",
            "train loss:0.011267111826573448\n",
            "train loss:0.04634960279301475\n",
            "train loss:0.061567632630297214\n",
            "train loss:0.0646947814696862\n",
            "train loss:0.025571928679475865\n",
            "train loss:0.021334858212187738\n",
            "train loss:0.02588724583410424\n",
            "train loss:0.024169294052878754\n",
            "train loss:0.04926335948224605\n",
            "train loss:0.03282692698045949\n",
            "train loss:0.04229058080739812\n",
            "train loss:0.01608567252264642\n",
            "=== epoch:18, train acc:0.992, test acc:0.954 ===\n",
            "train loss:0.012845680570698192\n",
            "train loss:0.06644288069928257\n",
            "train loss:0.01463044872479409\n",
            "train loss:0.012010300265246264\n",
            "train loss:0.01353118772069347\n",
            "train loss:0.007219698224468701\n",
            "train loss:0.013466649602376477\n",
            "train loss:0.05993580610073652\n",
            "train loss:0.04016762541037034\n",
            "train loss:0.01993236858753678\n",
            "train loss:0.019873902255491106\n",
            "train loss:0.024961152350508456\n",
            "train loss:0.034622962546898825\n",
            "train loss:0.015399100744634617\n",
            "train loss:0.029661249684662702\n",
            "train loss:0.021308077967642936\n",
            "train loss:0.027440482554205014\n",
            "train loss:0.0728951727339587\n",
            "train loss:0.02900547865099197\n",
            "train loss:0.05291947993355347\n",
            "train loss:0.012424196289152502\n",
            "train loss:0.01610137064236402\n",
            "train loss:0.06018857986250046\n",
            "train loss:0.04637359458261263\n",
            "train loss:0.018546146042919907\n",
            "train loss:0.022605610418678118\n",
            "train loss:0.017514025820691684\n",
            "train loss:0.006624089562103847\n",
            "train loss:0.09835590307995945\n",
            "train loss:0.045082485263726976\n",
            "train loss:0.04903675744946954\n",
            "train loss:0.010367089232796918\n",
            "train loss:0.044796689254869106\n",
            "train loss:0.013801196937553327\n",
            "train loss:0.0383269091820936\n",
            "train loss:0.023288498583021674\n",
            "train loss:0.02967005927452133\n",
            "train loss:0.043850418838876415\n",
            "train loss:0.01941332730065255\n",
            "train loss:0.045168418743650615\n",
            "train loss:0.03258098787062317\n",
            "train loss:0.015901460142124887\n",
            "train loss:0.017988531010293733\n",
            "train loss:0.01324730845583741\n",
            "train loss:0.0641610502912064\n",
            "train loss:0.02087029618411251\n",
            "train loss:0.015067752001894273\n",
            "train loss:0.025415721069758267\n",
            "train loss:0.030554118473731285\n",
            "train loss:0.06101428377523464\n",
            "=== epoch:19, train acc:0.987, test acc:0.953 ===\n",
            "train loss:0.01994531738949742\n",
            "train loss:0.01865985635431057\n",
            "train loss:0.014829694094621655\n",
            "train loss:0.05328910271407532\n",
            "train loss:0.08029574909364999\n",
            "train loss:0.054075823848646294\n",
            "train loss:0.013941820169280732\n",
            "train loss:0.008106029139043007\n",
            "train loss:0.03059751460616563\n",
            "train loss:0.010676103970617516\n",
            "train loss:0.029928691075872565\n",
            "train loss:0.05392607220153506\n",
            "train loss:0.0595069807939217\n",
            "train loss:0.020961963449102967\n",
            "train loss:0.007404999375613456\n",
            "train loss:0.026405220004358375\n",
            "train loss:0.062207901757791555\n",
            "train loss:0.022640149090978053\n",
            "train loss:0.02193745650899924\n",
            "train loss:0.010749568459921928\n",
            "train loss:0.02975226243244539\n",
            "train loss:0.040189046136526116\n",
            "train loss:0.0180250573125482\n",
            "train loss:0.028213263796047928\n",
            "train loss:0.006533753833128094\n",
            "train loss:0.023447520507455674\n",
            "train loss:0.0169360627399507\n",
            "train loss:0.01699351952365903\n",
            "train loss:0.018550320161076814\n",
            "train loss:0.019101221693100547\n",
            "train loss:0.01632961787894873\n",
            "train loss:0.025121329855059767\n",
            "train loss:0.020442256659778266\n",
            "train loss:0.04488912194936537\n",
            "train loss:0.04051503880931544\n",
            "train loss:0.02898338950668043\n",
            "train loss:0.0759566524858629\n",
            "train loss:0.010647839086085962\n",
            "train loss:0.0149095764449512\n",
            "train loss:0.026110986046570507\n",
            "train loss:0.012665919320132757\n",
            "train loss:0.03660124930525374\n",
            "train loss:0.026688666533809434\n",
            "train loss:0.07063028186806913\n",
            "train loss:0.017046201679082208\n",
            "train loss:0.020882648087684686\n",
            "train loss:0.01191571344091059\n",
            "train loss:0.01411919738454256\n",
            "train loss:0.04062926105422766\n",
            "train loss:0.017608978422214466\n",
            "=== epoch:20, train acc:0.987, test acc:0.951 ===\n",
            "train loss:0.012866202564139206\n",
            "train loss:0.042283856283184346\n",
            "train loss:0.026040366408324824\n",
            "train loss:0.03157858636503856\n",
            "train loss:0.00809291288335632\n",
            "train loss:0.009065082223367688\n",
            "train loss:0.009194968804514956\n",
            "train loss:0.050361921524940484\n",
            "train loss:0.021038831939775845\n",
            "train loss:0.01772181978371675\n",
            "train loss:0.027529029780823423\n",
            "train loss:0.022693252396082723\n",
            "train loss:0.007233209251741131\n",
            "train loss:0.034553030056225516\n",
            "train loss:0.013281233524600679\n",
            "train loss:0.014614934871081049\n",
            "train loss:0.016541765337407144\n",
            "train loss:0.011168827419240489\n",
            "train loss:0.05098680921232243\n",
            "train loss:0.02550763581698688\n",
            "train loss:0.01949361328527636\n",
            "train loss:0.01731266048121222\n",
            "train loss:0.013438715798645508\n",
            "train loss:0.018828319080337285\n",
            "train loss:0.01690323057614932\n",
            "train loss:0.016678936916999683\n",
            "train loss:0.011578109238386909\n",
            "train loss:0.01560899903007306\n",
            "train loss:0.016134836282784917\n",
            "train loss:0.040613962207790226\n",
            "train loss:0.013414562524162019\n",
            "train loss:0.0230861061738652\n",
            "train loss:0.02911857767202829\n",
            "train loss:0.022180248838285115\n",
            "train loss:0.015743989244096004\n",
            "train loss:0.02707271934987838\n",
            "train loss:0.017516809222447347\n",
            "train loss:0.017249778000793744\n",
            "train loss:0.02286432056388895\n",
            "train loss:0.06868165539816487\n",
            "train loss:0.008017951913256393\n",
            "train loss:0.007043813578439458\n",
            "train loss:0.012176328761881725\n",
            "train loss:0.0217534289617828\n",
            "train loss:0.01057521298424709\n",
            "train loss:0.019364490680758475\n",
            "train loss:0.026889309887326317\n",
            "train loss:0.010933924360436347\n",
            "train loss:0.006363271663456143\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.955\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}