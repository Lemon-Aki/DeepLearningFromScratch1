{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8SJHVEUwYpYVT0lB5foZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lemon-Aki/DeepLearningFromScratch1/blob/main/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrCvNbs8MVwq",
        "outputId": "03d54cdc-fd7b-479e-fc7d-f0cb6240c36e"
      },
      "source": [
        "# 코랩과 구글드라이드를 연동(인증 필요)\n",
        "#Transport endpoint is not connected 에러시 코랩 재연결\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/MyDrive/DeepLearningFromScratch1/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/DeepLearningFromScratch1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiXYytXQNi_B"
      },
      "source": [
        "#신경망 학습의 목적 : 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는것\n",
        "#최적화 : 매개변수의 최적값을 찾는 문제를 푸는것\n",
        "#확률적 경사 하강법(SGD, Stochastic Gradient Descent): 매개변수의 기울기(미분)을 구해, 가장 크게 기울어진 방향으로 매개변수값을 갱신하는 것을 반복\n",
        "#SGD 단점 : 비등방성 함수(방향에따라 성질(기울기)가 달라지는 함수)에서는 경색 탐로가 비효율적, 기울어진 방향이 본래의 최솟값과 다른 방향을 가리키는 점\n",
        "class GSD:\n",
        "  def __init__(self, lr=0.01):  #lr:학습률\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):  #params:가중치, grads:기울기(미분값)\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_W-xisWOx9n"
      },
      "source": [
        "#모멘텀(Momentum) : 운동량, 공이 그릇의 곡면을 따라 구르듯 움직이는 이미지\n",
        "#v <- αv - n(학습률) * ∂L/∂W (W(갱신할 가중치 매개변수)에 대한 손실함수의 기울기)\n",
        "#W <- W + v(v:velocity, 물리에서 말하는 속도)\n",
        "#αv항:물체가 아무런 힘을 받지 않을때 서서히 하강시키는 역할(지면 마찰, 공기저항 등)\n",
        "class Momentum:\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "    \n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "      params[key] += self.v[key]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmpXWDwmSMRx"
      },
      "source": [
        "#학습률 : 너무 작으면 학습 시간이 길어지고 너무 크면 발산해서 학습이 안됌\n",
        "#학습률 감소(learning rate decay) : 학습을 진행하면서 학습률을 줄여나가는 방법\n",
        "#AdaGrad(Adaptive Gradient): 각각의 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행, 과거의 기울기를 제곱하여 계속 더해감\n",
        "#예)y축 방향으로 기울기가 클때 처음에는 크게 움직여도, 갱신정도도 큰 폭으로 작아지도록 조정되어, y축방향으로는 갱신 강도가 빠르게 약해짐\n",
        "#RMSProp(Root Mean Square Propatation):AdaGrad를 무한히 하면 어느순간 갱신량이 0이 되는 문제를개선,\n",
        "#먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영하는 지수이동평균(EMA)를 사용\n",
        "class AdaGrad:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "    self.h = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      \n",
        "      for key, val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.h[key] += grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) #self.h[key]에 0이더라도 0으로 나누는 사태를 막아줌"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Ndpx4b6n83"
      },
      "source": [
        "#Adam(Adaptive Moment Estimation) 알고리즘 : 모멘텀과 RMSprop을 섞어놓은 최적화 알고리즘, 딥러닝에서 가장 흔히 사용되는 최적화 알고리즘\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "vpyNtvFx-B3h",
        "outputId": "5784dff5-3a5d-4b43-cb83-8e957b3003db"
      },
      "source": [
        "#가중치의 초깃값\n",
        "#가중치 감소(weight decay) 기법 : 오버피팅을 억제해 범용 성능을 높이는 테크닉, 가중치 값을 작게하여 과대적합을 피하기\n",
        "#가중치의 초기값을 0으로 하면 가중치가 고르게 되어, 곱셈 노드의 역전파에서 같은값이 유지가 되어 가중치를 여러개 갖는 의미가 없음\n",
        "#은닉층의 활성화함수의 출력 데이터의 분포를 관찰\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "    \n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    #기울기 소실(gradient vanishing) 문제 : 데이터가 0과 1에 치우쳐 분포하게 되어 역전파의 기울기 값이 점점 작아지다 사라짐\n",
        "    # w = np.random.randn(node_num, node_num) * 1\n",
        "    #표현력 제한 문제 : 활성화값이 한쪽에 치우쳐져서, 뉴런을 여러개 둔 의미가 없음\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    #Xavier초깃값: 좌우 대칭이라 중앙 부근이 선형인 함수의 권장 초깃값, 앞 계층의 노드가 n개일 경우, 표준편차가 1/root(n)인 분포를 사용할 것\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    #He초깃값: ReLU함수를 활성화 함수를 사용할 경우의 권장 초깃값, 앞 계층의 노드가 n개일 경우, 표준편차가 root(2/n)인 정규분포 사용\n",
        "    w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    # z = sigmoid(a)\n",
        "    z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8A5WC91CNYg"
      },
      "source": [
        "#가중치의 초깃값을 적절히 설정하면 각 층의 활성화값 분포가 적당히 퍼지면서 학습이 원할히 수행됨\n",
        "#배치 정규화(Batch Normalization): 각 층이 활성화를 적당히 퍼뜨리도록 강제하기위한 아이디어, 미니배치 단위로 정규화\n",
        "#장점1 : 학습속도가 빠름\n",
        "#장점2 : 초깃값에 크게 의존하지 않는다\n",
        "#장점3 : 과대적합를 억제"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9it12loYFco8",
        "outputId": "888b7b5f-029b-4771-f83d-a8d152d97f09"
      },
      "source": [
        "#오버피팅(과대적합): 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태\n",
        "#원인1 : 매개변수가 많고, 표현력이 높은 모델\n",
        "#원인2 : 훈련 데이터가 적음\n",
        "#가중치 감소 : 학습 과정에서 큰 가중치에 대해 그에 상응하는 큰 페널티를 부과하는 방법\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)  #에폭단위의 훈련 정확도 저장\n",
        "        test_acc = network.accuracy(x_test, t_test)   #에폭단위의 테스트 정확도 저장\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, train acc:0.11333333333333333, test acc:0.1189\n",
            "epoch:1, train acc:0.13666666666666666, test acc:0.1347\n",
            "epoch:2, train acc:0.14, test acc:0.1441\n",
            "epoch:3, train acc:0.17, test acc:0.1538\n",
            "epoch:4, train acc:0.17666666666666667, test acc:0.1599\n",
            "epoch:5, train acc:0.19333333333333333, test acc:0.169\n",
            "epoch:6, train acc:0.21, test acc:0.1811\n",
            "epoch:7, train acc:0.21333333333333335, test acc:0.1831\n",
            "epoch:8, train acc:0.23666666666666666, test acc:0.1888\n",
            "epoch:9, train acc:0.2733333333333333, test acc:0.1939\n",
            "epoch:10, train acc:0.2966666666666667, test acc:0.2072\n",
            "epoch:11, train acc:0.31, test acc:0.2114\n",
            "epoch:12, train acc:0.33, test acc:0.2176\n",
            "epoch:13, train acc:0.35, test acc:0.2299\n",
            "epoch:14, train acc:0.36333333333333334, test acc:0.2424\n",
            "epoch:15, train acc:0.38, test acc:0.2526\n",
            "epoch:16, train acc:0.38, test acc:0.2596\n",
            "epoch:17, train acc:0.38333333333333336, test acc:0.268\n",
            "epoch:18, train acc:0.39666666666666667, test acc:0.2763\n",
            "epoch:19, train acc:0.3933333333333333, test acc:0.2818\n",
            "epoch:20, train acc:0.4066666666666667, test acc:0.2916\n",
            "epoch:21, train acc:0.4266666666666667, test acc:0.299\n",
            "epoch:22, train acc:0.44666666666666666, test acc:0.3109\n",
            "epoch:23, train acc:0.4766666666666667, test acc:0.3267\n",
            "epoch:24, train acc:0.48333333333333334, test acc:0.3309\n",
            "epoch:25, train acc:0.4666666666666667, test acc:0.3367\n",
            "epoch:26, train acc:0.49666666666666665, test acc:0.3488\n",
            "epoch:27, train acc:0.49666666666666665, test acc:0.3446\n",
            "epoch:28, train acc:0.49666666666666665, test acc:0.3528\n",
            "epoch:29, train acc:0.51, test acc:0.3637\n",
            "epoch:30, train acc:0.5333333333333333, test acc:0.3727\n",
            "epoch:31, train acc:0.53, test acc:0.3827\n",
            "epoch:32, train acc:0.5633333333333334, test acc:0.4\n",
            "epoch:33, train acc:0.57, test acc:0.4075\n",
            "epoch:34, train acc:0.59, test acc:0.4151\n",
            "epoch:35, train acc:0.5833333333333334, test acc:0.4144\n",
            "epoch:36, train acc:0.5766666666666667, test acc:0.4216\n",
            "epoch:37, train acc:0.6133333333333333, test acc:0.4343\n",
            "epoch:38, train acc:0.5933333333333334, test acc:0.4272\n",
            "epoch:39, train acc:0.5966666666666667, test acc:0.4312\n",
            "epoch:40, train acc:0.5833333333333334, test acc:0.4405\n",
            "epoch:41, train acc:0.6266666666666667, test acc:0.4528\n",
            "epoch:42, train acc:0.63, test acc:0.4588\n",
            "epoch:43, train acc:0.63, test acc:0.462\n",
            "epoch:44, train acc:0.6366666666666667, test acc:0.4699\n",
            "epoch:45, train acc:0.66, test acc:0.4806\n",
            "epoch:46, train acc:0.6766666666666666, test acc:0.4964\n",
            "epoch:47, train acc:0.6866666666666666, test acc:0.5023\n",
            "epoch:48, train acc:0.6833333333333333, test acc:0.5122\n",
            "epoch:49, train acc:0.6966666666666667, test acc:0.52\n",
            "epoch:50, train acc:0.7033333333333334, test acc:0.5202\n",
            "epoch:51, train acc:0.7133333333333334, test acc:0.5246\n",
            "epoch:52, train acc:0.6766666666666666, test acc:0.503\n",
            "epoch:53, train acc:0.6966666666666667, test acc:0.5147\n",
            "epoch:54, train acc:0.7366666666666667, test acc:0.5352\n",
            "epoch:55, train acc:0.7233333333333334, test acc:0.5445\n",
            "epoch:56, train acc:0.7333333333333333, test acc:0.5488\n",
            "epoch:57, train acc:0.7166666666666667, test acc:0.5489\n",
            "epoch:58, train acc:0.7333333333333333, test acc:0.5546\n",
            "epoch:59, train acc:0.7366666666666667, test acc:0.5503\n",
            "epoch:60, train acc:0.7433333333333333, test acc:0.5572\n",
            "epoch:61, train acc:0.7366666666666667, test acc:0.5582\n",
            "epoch:62, train acc:0.7266666666666667, test acc:0.5622\n",
            "epoch:63, train acc:0.71, test acc:0.5314\n",
            "epoch:64, train acc:0.7066666666666667, test acc:0.5382\n",
            "epoch:65, train acc:0.74, test acc:0.5617\n",
            "epoch:66, train acc:0.75, test acc:0.5785\n",
            "epoch:67, train acc:0.7466666666666667, test acc:0.5671\n",
            "epoch:68, train acc:0.7533333333333333, test acc:0.571\n",
            "epoch:69, train acc:0.7766666666666666, test acc:0.5843\n",
            "epoch:70, train acc:0.77, test acc:0.5881\n",
            "epoch:71, train acc:0.7833333333333333, test acc:0.6001\n",
            "epoch:72, train acc:0.7666666666666667, test acc:0.5991\n",
            "epoch:73, train acc:0.76, test acc:0.5834\n",
            "epoch:74, train acc:0.8, test acc:0.5987\n",
            "epoch:75, train acc:0.7733333333333333, test acc:0.5852\n",
            "epoch:76, train acc:0.7766666666666666, test acc:0.5903\n",
            "epoch:77, train acc:0.7866666666666666, test acc:0.6139\n",
            "epoch:78, train acc:0.8066666666666666, test acc:0.6081\n",
            "epoch:79, train acc:0.79, test acc:0.5969\n",
            "epoch:80, train acc:0.8, test acc:0.5955\n",
            "epoch:81, train acc:0.8166666666666667, test acc:0.6221\n",
            "epoch:82, train acc:0.82, test acc:0.6333\n",
            "epoch:83, train acc:0.7566666666666667, test acc:0.596\n",
            "epoch:84, train acc:0.8, test acc:0.6172\n",
            "epoch:85, train acc:0.81, test acc:0.6242\n",
            "epoch:86, train acc:0.8166666666666667, test acc:0.6376\n",
            "epoch:87, train acc:0.8, test acc:0.6324\n",
            "epoch:88, train acc:0.8133333333333334, test acc:0.6336\n",
            "epoch:89, train acc:0.8166666666666667, test acc:0.6299\n",
            "epoch:90, train acc:0.8266666666666667, test acc:0.6432\n",
            "epoch:91, train acc:0.8266666666666667, test acc:0.6369\n",
            "epoch:92, train acc:0.8233333333333334, test acc:0.6311\n",
            "epoch:93, train acc:0.8166666666666667, test acc:0.6337\n",
            "epoch:94, train acc:0.82, test acc:0.6355\n",
            "epoch:95, train acc:0.82, test acc:0.6428\n",
            "epoch:96, train acc:0.8133333333333334, test acc:0.6195\n",
            "epoch:97, train acc:0.8433333333333334, test acc:0.6394\n",
            "epoch:98, train acc:0.8333333333333334, test acc:0.6336\n",
            "epoch:99, train acc:0.8333333333333334, test acc:0.6355\n",
            "epoch:100, train acc:0.84, test acc:0.6473\n",
            "epoch:101, train acc:0.8433333333333334, test acc:0.6528\n",
            "epoch:102, train acc:0.85, test acc:0.6557\n",
            "epoch:103, train acc:0.8466666666666667, test acc:0.6559\n",
            "epoch:104, train acc:0.8366666666666667, test acc:0.6352\n",
            "epoch:105, train acc:0.8366666666666667, test acc:0.6513\n",
            "epoch:106, train acc:0.84, test acc:0.6446\n",
            "epoch:107, train acc:0.8333333333333334, test acc:0.6567\n",
            "epoch:108, train acc:0.8466666666666667, test acc:0.6562\n",
            "epoch:109, train acc:0.8433333333333334, test acc:0.6585\n",
            "epoch:110, train acc:0.8466666666666667, test acc:0.655\n",
            "epoch:111, train acc:0.8633333333333333, test acc:0.6561\n",
            "epoch:112, train acc:0.84, test acc:0.6535\n",
            "epoch:113, train acc:0.8333333333333334, test acc:0.648\n",
            "epoch:114, train acc:0.8433333333333334, test acc:0.6578\n",
            "epoch:115, train acc:0.8333333333333334, test acc:0.6525\n",
            "epoch:116, train acc:0.84, test acc:0.661\n",
            "epoch:117, train acc:0.8466666666666667, test acc:0.6478\n",
            "epoch:118, train acc:0.86, test acc:0.6543\n",
            "epoch:119, train acc:0.8533333333333334, test acc:0.651\n",
            "epoch:120, train acc:0.85, test acc:0.6583\n",
            "epoch:121, train acc:0.85, test acc:0.6651\n",
            "epoch:122, train acc:0.8533333333333334, test acc:0.6611\n",
            "epoch:123, train acc:0.86, test acc:0.6599\n",
            "epoch:124, train acc:0.8566666666666667, test acc:0.6516\n",
            "epoch:125, train acc:0.86, test acc:0.6568\n",
            "epoch:126, train acc:0.86, test acc:0.6654\n",
            "epoch:127, train acc:0.8566666666666667, test acc:0.6686\n",
            "epoch:128, train acc:0.85, test acc:0.6622\n",
            "epoch:129, train acc:0.87, test acc:0.6692\n",
            "epoch:130, train acc:0.87, test acc:0.6663\n",
            "epoch:131, train acc:0.8533333333333334, test acc:0.6661\n",
            "epoch:132, train acc:0.86, test acc:0.6602\n",
            "epoch:133, train acc:0.8666666666666667, test acc:0.6652\n",
            "epoch:134, train acc:0.8733333333333333, test acc:0.6712\n",
            "epoch:135, train acc:0.8666666666666667, test acc:0.6721\n",
            "epoch:136, train acc:0.8633333333333333, test acc:0.6778\n",
            "epoch:137, train acc:0.86, test acc:0.675\n",
            "epoch:138, train acc:0.8566666666666667, test acc:0.6729\n",
            "epoch:139, train acc:0.8633333333333333, test acc:0.6726\n",
            "epoch:140, train acc:0.8566666666666667, test acc:0.6726\n",
            "epoch:141, train acc:0.85, test acc:0.6697\n",
            "epoch:142, train acc:0.85, test acc:0.6711\n",
            "epoch:143, train acc:0.86, test acc:0.6739\n",
            "epoch:144, train acc:0.85, test acc:0.6669\n",
            "epoch:145, train acc:0.8633333333333333, test acc:0.6582\n",
            "epoch:146, train acc:0.86, test acc:0.6712\n",
            "epoch:147, train acc:0.85, test acc:0.6735\n",
            "epoch:148, train acc:0.86, test acc:0.6684\n",
            "epoch:149, train acc:0.8633333333333333, test acc:0.676\n",
            "epoch:150, train acc:0.8633333333333333, test acc:0.6755\n",
            "epoch:151, train acc:0.86, test acc:0.6638\n",
            "epoch:152, train acc:0.86, test acc:0.6659\n",
            "epoch:153, train acc:0.8633333333333333, test acc:0.6549\n",
            "epoch:154, train acc:0.87, test acc:0.6655\n",
            "epoch:155, train acc:0.86, test acc:0.6698\n",
            "epoch:156, train acc:0.8666666666666667, test acc:0.6752\n",
            "epoch:157, train acc:0.8533333333333334, test acc:0.6662\n",
            "epoch:158, train acc:0.8733333333333333, test acc:0.6669\n",
            "epoch:159, train acc:0.88, test acc:0.677\n",
            "epoch:160, train acc:0.8766666666666667, test acc:0.6795\n",
            "epoch:161, train acc:0.8566666666666667, test acc:0.6784\n",
            "epoch:162, train acc:0.8533333333333334, test acc:0.679\n",
            "epoch:163, train acc:0.8566666666666667, test acc:0.6772\n",
            "epoch:164, train acc:0.8533333333333334, test acc:0.6781\n",
            "epoch:165, train acc:0.8533333333333334, test acc:0.6563\n",
            "epoch:166, train acc:0.85, test acc:0.6705\n",
            "epoch:167, train acc:0.8566666666666667, test acc:0.6766\n",
            "epoch:168, train acc:0.87, test acc:0.6769\n",
            "epoch:169, train acc:0.8733333333333333, test acc:0.6747\n",
            "epoch:170, train acc:0.86, test acc:0.6756\n",
            "epoch:171, train acc:0.86, test acc:0.6781\n",
            "epoch:172, train acc:0.8466666666666667, test acc:0.6762\n",
            "epoch:173, train acc:0.87, test acc:0.67\n",
            "epoch:174, train acc:0.8766666666666667, test acc:0.6739\n",
            "epoch:175, train acc:0.8566666666666667, test acc:0.6785\n",
            "epoch:176, train acc:0.86, test acc:0.6784\n",
            "epoch:177, train acc:0.8666666666666667, test acc:0.6643\n",
            "epoch:178, train acc:0.8533333333333334, test acc:0.6653\n",
            "epoch:179, train acc:0.87, test acc:0.6709\n",
            "epoch:180, train acc:0.8733333333333333, test acc:0.6793\n",
            "epoch:181, train acc:0.8666666666666667, test acc:0.6792\n",
            "epoch:182, train acc:0.8666666666666667, test acc:0.6801\n",
            "epoch:183, train acc:0.8733333333333333, test acc:0.6845\n",
            "epoch:184, train acc:0.8666666666666667, test acc:0.6715\n",
            "epoch:185, train acc:0.8466666666666667, test acc:0.6579\n",
            "epoch:186, train acc:0.86, test acc:0.6705\n",
            "epoch:187, train acc:0.8566666666666667, test acc:0.6693\n",
            "epoch:188, train acc:0.8733333333333333, test acc:0.682\n",
            "epoch:189, train acc:0.88, test acc:0.6827\n",
            "epoch:190, train acc:0.8666666666666667, test acc:0.6733\n",
            "epoch:191, train acc:0.8533333333333334, test acc:0.672\n",
            "epoch:192, train acc:0.86, test acc:0.6724\n",
            "epoch:193, train acc:0.87, test acc:0.6716\n",
            "epoch:194, train acc:0.86, test acc:0.6781\n",
            "epoch:195, train acc:0.86, test acc:0.6763\n",
            "epoch:196, train acc:0.86, test acc:0.6747\n",
            "epoch:197, train acc:0.8633333333333333, test acc:0.6816\n",
            "epoch:198, train acc:0.8633333333333333, test acc:0.6759\n",
            "epoch:199, train acc:0.88, test acc:0.6836\n",
            "epoch:200, train acc:0.8666666666666667, test acc:0.6834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TVBY-MC1Gk0y",
        "outputId": "e7c4a3aa-9677-4b84-8941-02583af344f5"
      },
      "source": [
        "#드롭아웃(dropout): 신경망 모델이 복잡해서 가중치 감소만으로 대응이 어려울때 사용하는 오버피팅 억제 방식,\n",
        "#훈련때 은닉층의 뉴런을 무작위로 골라 삭제, 시험 때는 모든 뉴런을 사용\n",
        "#단, 시험때는 각 뉴런의 출력에 훈련때 삭제 안한 비율을 곱하여 출력\n",
        "#앙상블 학습(Ensemble learning): 새별적으로 학습시킨 여러 모델의 출력을 평균(또는 투표)내어 추론하는 방식\n",
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio = 0.15):\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, train_flg=True):\n",
        "    if train_flg:\n",
        "      #x와 형상이 같은 배열을 무작위 생성, dropout_ratio보다 큰 원소만 True로 설정\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "      return x * self.mask\n",
        "    else:\n",
        "      return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "  #순전파때 신호를 통과시키는 뉴런은 역전파 때도 신호를 통과, 순전파때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단\n",
        "  def backward(self, dour):\n",
        "    return dout * self.mask\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.298803180410301\n",
            "=== epoch:1, train acc:0.11666666666666667, test acc:0.0974 ===\n",
            "train loss:2.2981180873240263\n",
            "train loss:2.283073957619847\n",
            "train loss:2.290005901855268\n",
            "=== epoch:2, train acc:0.11333333333333333, test acc:0.0995 ===\n",
            "train loss:2.313253834074613\n",
            "train loss:2.3170738314395347\n",
            "train loss:2.296925797665952\n",
            "=== epoch:3, train acc:0.11, test acc:0.0992 ===\n",
            "train loss:2.2874682931864827\n",
            "train loss:2.3006357901663126\n",
            "train loss:2.3049942681134463\n",
            "=== epoch:4, train acc:0.11, test acc:0.1014 ===\n",
            "train loss:2.2904125247868135\n",
            "train loss:2.2966029129787384\n",
            "train loss:2.302914278421629\n",
            "=== epoch:5, train acc:0.11, test acc:0.1019 ===\n",
            "train loss:2.2872095002370023\n",
            "train loss:2.298315132704218\n",
            "train loss:2.294559647744601\n",
            "=== epoch:6, train acc:0.11666666666666667, test acc:0.1024 ===\n",
            "train loss:2.2962129560733304\n",
            "train loss:2.290340883462771\n",
            "train loss:2.287332471893301\n",
            "=== epoch:7, train acc:0.11333333333333333, test acc:0.1029 ===\n",
            "train loss:2.2947011908755606\n",
            "train loss:2.284635373840878\n",
            "train loss:2.2840609254938182\n",
            "=== epoch:8, train acc:0.11666666666666667, test acc:0.1048 ===\n",
            "train loss:2.295242902921804\n",
            "train loss:2.273347714205336\n",
            "train loss:2.298881900885848\n",
            "=== epoch:9, train acc:0.11666666666666667, test acc:0.1055 ===\n",
            "train loss:2.295569153963451\n",
            "train loss:2.2672631531649197\n",
            "train loss:2.2850499391073744\n",
            "=== epoch:10, train acc:0.11666666666666667, test acc:0.1052 ===\n",
            "train loss:2.291714269177851\n",
            "train loss:2.297219245861916\n",
            "train loss:2.283843027997513\n",
            "=== epoch:11, train acc:0.11666666666666667, test acc:0.1065 ===\n",
            "train loss:2.296103499019808\n",
            "train loss:2.2946491496958052\n",
            "train loss:2.3010916613404113\n",
            "=== epoch:12, train acc:0.11666666666666667, test acc:0.1058 ===\n",
            "train loss:2.2733473953555388\n",
            "train loss:2.2918986960197323\n",
            "train loss:2.2871804982738397\n",
            "=== epoch:13, train acc:0.12333333333333334, test acc:0.108 ===\n",
            "train loss:2.288420353174728\n",
            "train loss:2.2872012911192483\n",
            "train loss:2.2818796018516605\n",
            "=== epoch:14, train acc:0.13, test acc:0.1086 ===\n",
            "train loss:2.287634386408421\n",
            "train loss:2.286179923782067\n",
            "train loss:2.2894742116802886\n",
            "=== epoch:15, train acc:0.13333333333333333, test acc:0.109 ===\n",
            "train loss:2.2914777550260044\n",
            "train loss:2.301639503822794\n",
            "train loss:2.2819705879881824\n",
            "=== epoch:16, train acc:0.13333333333333333, test acc:0.1096 ===\n",
            "train loss:2.2683620084057607\n",
            "train loss:2.2884279252732527\n",
            "train loss:2.287734372377221\n",
            "=== epoch:17, train acc:0.13666666666666666, test acc:0.1113 ===\n",
            "train loss:2.280482447796177\n",
            "train loss:2.2872250046153813\n",
            "train loss:2.276063072156438\n",
            "=== epoch:18, train acc:0.13333333333333333, test acc:0.1097 ===\n",
            "train loss:2.2899813016454176\n",
            "train loss:2.2824347596898007\n",
            "train loss:2.2729314316323435\n",
            "=== epoch:19, train acc:0.13666666666666666, test acc:0.1109 ===\n",
            "train loss:2.2767051229510495\n",
            "train loss:2.2795355012085525\n",
            "train loss:2.2797865458316537\n",
            "=== epoch:20, train acc:0.14, test acc:0.1134 ===\n",
            "train loss:2.275177590022474\n",
            "train loss:2.278270447775973\n",
            "train loss:2.2660033233837997\n",
            "=== epoch:21, train acc:0.13666666666666666, test acc:0.1131 ===\n",
            "train loss:2.266576233019228\n",
            "train loss:2.281844366120658\n",
            "train loss:2.2724228958448807\n",
            "=== epoch:22, train acc:0.13666666666666666, test acc:0.114 ===\n",
            "train loss:2.2685290804298064\n",
            "train loss:2.2703223886291584\n",
            "train loss:2.2785036255222737\n",
            "=== epoch:23, train acc:0.13666666666666666, test acc:0.1161 ===\n",
            "train loss:2.2718248773999234\n",
            "train loss:2.2645200262097456\n",
            "train loss:2.2772683658356043\n",
            "=== epoch:24, train acc:0.13666666666666666, test acc:0.1173 ===\n",
            "train loss:2.2871554879713796\n",
            "train loss:2.275204885900736\n",
            "train loss:2.2624804439141015\n",
            "=== epoch:25, train acc:0.13666666666666666, test acc:0.118 ===\n",
            "train loss:2.2731919987661895\n",
            "train loss:2.28451768969679\n",
            "train loss:2.273827924864243\n",
            "=== epoch:26, train acc:0.14, test acc:0.1209 ===\n",
            "train loss:2.277717252232133\n",
            "train loss:2.2583902978836288\n",
            "train loss:2.267676944486415\n",
            "=== epoch:27, train acc:0.14, test acc:0.1206 ===\n",
            "train loss:2.2825827052177696\n",
            "train loss:2.2633271468288907\n",
            "train loss:2.2880243817181833\n",
            "=== epoch:28, train acc:0.14666666666666667, test acc:0.1239 ===\n",
            "train loss:2.277825185891586\n",
            "train loss:2.2636172499436547\n",
            "train loss:2.272461036371936\n",
            "=== epoch:29, train acc:0.14666666666666667, test acc:0.1241 ===\n",
            "train loss:2.2683897320832114\n",
            "train loss:2.275385574475654\n",
            "train loss:2.2602439711832276\n",
            "=== epoch:30, train acc:0.15666666666666668, test acc:0.1264 ===\n",
            "train loss:2.2642875501778086\n",
            "train loss:2.2625088641643156\n",
            "train loss:2.262943100359454\n",
            "=== epoch:31, train acc:0.15333333333333332, test acc:0.1268 ===\n",
            "train loss:2.272371512086562\n",
            "train loss:2.2628917164823936\n",
            "train loss:2.2724757240535998\n",
            "=== epoch:32, train acc:0.15666666666666668, test acc:0.1294 ===\n",
            "train loss:2.266452611429128\n",
            "train loss:2.2688351413826098\n",
            "train loss:2.2717146231376475\n",
            "=== epoch:33, train acc:0.16666666666666666, test acc:0.1329 ===\n",
            "train loss:2.271619204535624\n",
            "train loss:2.252953300235717\n",
            "train loss:2.262921591146195\n",
            "=== epoch:34, train acc:0.16333333333333333, test acc:0.1352 ===\n",
            "train loss:2.2692341054679663\n",
            "train loss:2.262889169731313\n",
            "train loss:2.270836580153551\n",
            "=== epoch:35, train acc:0.17333333333333334, test acc:0.1395 ===\n",
            "train loss:2.259448307233219\n",
            "train loss:2.258444859542659\n",
            "train loss:2.2503102456189423\n",
            "=== epoch:36, train acc:0.17666666666666667, test acc:0.1429 ===\n",
            "train loss:2.2534304946321186\n",
            "train loss:2.2477964258562935\n",
            "train loss:2.2583923180034247\n",
            "=== epoch:37, train acc:0.17666666666666667, test acc:0.1438 ===\n",
            "train loss:2.2559547958109345\n",
            "train loss:2.255064841988597\n",
            "train loss:2.2499901482275173\n",
            "=== epoch:38, train acc:0.18, test acc:0.1454 ===\n",
            "train loss:2.258815055441404\n",
            "train loss:2.264276589328149\n",
            "train loss:2.2383191099881152\n",
            "=== epoch:39, train acc:0.18333333333333332, test acc:0.1485 ===\n",
            "train loss:2.2535338081297804\n",
            "train loss:2.2452087974821304\n",
            "train loss:2.252629357711334\n",
            "=== epoch:40, train acc:0.18333333333333332, test acc:0.1508 ===\n",
            "train loss:2.259017717357692\n",
            "train loss:2.2324138502316475\n",
            "train loss:2.2610781427651294\n",
            "=== epoch:41, train acc:0.2, test acc:0.1532 ===\n",
            "train loss:2.242420662715459\n",
            "train loss:2.254324322742865\n",
            "train loss:2.2421975555773135\n",
            "=== epoch:42, train acc:0.19666666666666666, test acc:0.1547 ===\n",
            "train loss:2.250256265746387\n",
            "train loss:2.261883984054828\n",
            "train loss:2.23550636375286\n",
            "=== epoch:43, train acc:0.19333333333333333, test acc:0.1568 ===\n",
            "train loss:2.2699864686806164\n",
            "train loss:2.2443781788668993\n",
            "train loss:2.2333528262002877\n",
            "=== epoch:44, train acc:0.2, test acc:0.1588 ===\n",
            "train loss:2.219941932536913\n",
            "train loss:2.250459618114566\n",
            "train loss:2.2417472756013344\n",
            "=== epoch:45, train acc:0.2, test acc:0.1623 ===\n",
            "train loss:2.2576215191430773\n",
            "train loss:2.2508652436166354\n",
            "train loss:2.2433676507154248\n",
            "=== epoch:46, train acc:0.2, test acc:0.1657 ===\n",
            "train loss:2.2405268806043104\n",
            "train loss:2.2469025951837334\n",
            "train loss:2.228393748650324\n",
            "=== epoch:47, train acc:0.2, test acc:0.1673 ===\n",
            "train loss:2.2395886917983274\n",
            "train loss:2.252520730477519\n",
            "train loss:2.2438934428216486\n",
            "=== epoch:48, train acc:0.20333333333333334, test acc:0.1715 ===\n",
            "train loss:2.223011006891472\n",
            "train loss:2.248251192251953\n",
            "train loss:2.2604639977333782\n",
            "=== epoch:49, train acc:0.21, test acc:0.1756 ===\n",
            "train loss:2.2150737316579407\n",
            "train loss:2.230726497974565\n",
            "train loss:2.2492266607917855\n",
            "=== epoch:50, train acc:0.21, test acc:0.176 ===\n",
            "train loss:2.23638381135636\n",
            "train loss:2.240724875108537\n",
            "train loss:2.2383841897120895\n",
            "=== epoch:51, train acc:0.21333333333333335, test acc:0.1789 ===\n",
            "train loss:2.24084317367319\n",
            "train loss:2.251131627612893\n",
            "train loss:2.2118149428985525\n",
            "=== epoch:52, train acc:0.21333333333333335, test acc:0.1799 ===\n",
            "train loss:2.237047260406923\n",
            "train loss:2.2516225734540374\n",
            "train loss:2.2595736158938537\n",
            "=== epoch:53, train acc:0.22, test acc:0.1845 ===\n",
            "train loss:2.2548512672764094\n",
            "train loss:2.2142740708636417\n",
            "train loss:2.237247345597426\n",
            "=== epoch:54, train acc:0.23333333333333334, test acc:0.1892 ===\n",
            "train loss:2.2452703764182536\n",
            "train loss:2.235650122506091\n",
            "train loss:2.222207598896398\n",
            "=== epoch:55, train acc:0.23, test acc:0.1906 ===\n",
            "train loss:2.227834180397598\n",
            "train loss:2.200159184214936\n",
            "train loss:2.2233328523779905\n",
            "=== epoch:56, train acc:0.22666666666666666, test acc:0.1893 ===\n",
            "train loss:2.2454029701664986\n",
            "train loss:2.2274935314810036\n",
            "train loss:2.208800597670629\n",
            "=== epoch:57, train acc:0.24, test acc:0.1954 ===\n",
            "train loss:2.236153547654558\n",
            "train loss:2.2132498204857916\n",
            "train loss:2.2210029057305958\n",
            "=== epoch:58, train acc:0.23333333333333334, test acc:0.1953 ===\n",
            "train loss:2.252491888031754\n",
            "train loss:2.2373219644431943\n",
            "train loss:2.212029445598172\n",
            "=== epoch:59, train acc:0.24666666666666667, test acc:0.2008 ===\n",
            "train loss:2.219606125894964\n",
            "train loss:2.225127027891059\n",
            "train loss:2.2379540392244053\n",
            "=== epoch:60, train acc:0.25333333333333335, test acc:0.201 ===\n",
            "train loss:2.243783641763835\n",
            "train loss:2.248313037080208\n",
            "train loss:2.228732170769293\n",
            "=== epoch:61, train acc:0.26, test acc:0.2079 ===\n",
            "train loss:2.2462884004226824\n",
            "train loss:2.232275233626658\n",
            "train loss:2.2361825415517926\n",
            "=== epoch:62, train acc:0.27, test acc:0.2159 ===\n",
            "train loss:2.207978079908136\n",
            "train loss:2.2077432400983446\n",
            "train loss:2.2247935328101964\n",
            "=== epoch:63, train acc:0.27, test acc:0.2188 ===\n",
            "train loss:2.229622003234849\n",
            "train loss:2.203395850614081\n",
            "train loss:2.196191977534411\n",
            "=== epoch:64, train acc:0.27666666666666667, test acc:0.2253 ===\n",
            "train loss:2.2305428773715557\n",
            "train loss:2.216397192836363\n",
            "train loss:2.193751570312232\n",
            "=== epoch:65, train acc:0.29333333333333333, test acc:0.2327 ===\n",
            "train loss:2.2280360893290863\n",
            "train loss:2.2025115709715894\n",
            "train loss:2.2353391339224364\n",
            "=== epoch:66, train acc:0.29, test acc:0.2358 ===\n",
            "train loss:2.1954157745402485\n",
            "train loss:2.2220392070338986\n",
            "train loss:2.2237563458815433\n",
            "=== epoch:67, train acc:0.28, test acc:0.2335 ===\n",
            "train loss:2.222696996129112\n",
            "train loss:2.2218313639790708\n",
            "train loss:2.2385894002422897\n",
            "=== epoch:68, train acc:0.2833333333333333, test acc:0.2366 ===\n",
            "train loss:2.207776153681668\n",
            "train loss:2.2077117947325253\n",
            "train loss:2.2142765572788097\n",
            "=== epoch:69, train acc:0.29333333333333333, test acc:0.2401 ===\n",
            "train loss:2.2235318581323487\n",
            "train loss:2.1865426228535285\n",
            "train loss:2.2353298385183678\n",
            "=== epoch:70, train acc:0.3, test acc:0.2467 ===\n",
            "train loss:2.2146697818367356\n",
            "train loss:2.213450433098621\n",
            "train loss:2.1759829525985457\n",
            "=== epoch:71, train acc:0.2966666666666667, test acc:0.2432 ===\n",
            "train loss:2.211112192023509\n",
            "train loss:2.2257413385333082\n",
            "train loss:2.218259474747765\n",
            "=== epoch:72, train acc:0.2966666666666667, test acc:0.2453 ===\n",
            "train loss:2.223078623905733\n",
            "train loss:2.23995113360506\n",
            "train loss:2.2374680478320683\n",
            "=== epoch:73, train acc:0.30333333333333334, test acc:0.259 ===\n",
            "train loss:2.2241928821262467\n",
            "train loss:2.2298284717630503\n",
            "train loss:2.2267532046921312\n",
            "=== epoch:74, train acc:0.3, test acc:0.2627 ===\n",
            "train loss:2.213658660947787\n",
            "train loss:2.1834955966392338\n",
            "train loss:2.187605129956523\n",
            "=== epoch:75, train acc:0.31333333333333335, test acc:0.2659 ===\n",
            "train loss:2.222162837780841\n",
            "train loss:2.2231448588789795\n",
            "train loss:2.1941058678043626\n",
            "=== epoch:76, train acc:0.33, test acc:0.2726 ===\n",
            "train loss:2.181368893143553\n",
            "train loss:2.1979191611886533\n",
            "train loss:2.18359120385537\n",
            "=== epoch:77, train acc:0.31666666666666665, test acc:0.2632 ===\n",
            "train loss:2.2082862036112703\n",
            "train loss:2.1972524911354743\n",
            "train loss:2.2149317091047114\n",
            "=== epoch:78, train acc:0.32, test acc:0.2655 ===\n",
            "train loss:2.210937749372764\n",
            "train loss:2.2047144614837975\n",
            "train loss:2.2060566080058077\n",
            "=== epoch:79, train acc:0.3233333333333333, test acc:0.2603 ===\n",
            "train loss:2.200769015413405\n",
            "train loss:2.1962048458489005\n",
            "train loss:2.1902434542824794\n",
            "=== epoch:80, train acc:0.32666666666666666, test acc:0.2619 ===\n",
            "train loss:2.220955455685918\n",
            "train loss:2.1838346323678937\n",
            "train loss:2.1778741430227107\n",
            "=== epoch:81, train acc:0.3233333333333333, test acc:0.2622 ===\n",
            "train loss:2.1815253381649633\n",
            "train loss:2.1729898760645723\n",
            "train loss:2.1635964377240198\n",
            "=== epoch:82, train acc:0.32666666666666666, test acc:0.2614 ===\n",
            "train loss:2.1918850234220097\n",
            "train loss:2.1502636461529967\n",
            "train loss:2.191307931025008\n",
            "=== epoch:83, train acc:0.31666666666666665, test acc:0.2579 ===\n",
            "train loss:2.150312427539844\n",
            "train loss:2.1746117880580673\n",
            "train loss:2.191467045596234\n",
            "=== epoch:84, train acc:0.31666666666666665, test acc:0.2539 ===\n",
            "train loss:2.1742618313139404\n",
            "train loss:2.1753919400418726\n",
            "train loss:2.1662111967333892\n",
            "=== epoch:85, train acc:0.33, test acc:0.2638 ===\n",
            "train loss:2.192125495746904\n",
            "train loss:2.1592282275310937\n",
            "train loss:2.186173638270733\n",
            "=== epoch:86, train acc:0.33, test acc:0.2647 ===\n",
            "train loss:2.2047773129215456\n",
            "train loss:2.16711939576481\n",
            "train loss:2.16693523137032\n",
            "=== epoch:87, train acc:0.33666666666666667, test acc:0.2703 ===\n",
            "train loss:2.1801378229957997\n",
            "train loss:2.1682179304968647\n",
            "train loss:2.1565959338069156\n",
            "=== epoch:88, train acc:0.3433333333333333, test acc:0.2744 ===\n",
            "train loss:2.1910662836814883\n",
            "train loss:2.1824379063193002\n",
            "train loss:2.1697066832884384\n",
            "=== epoch:89, train acc:0.34, test acc:0.28 ===\n",
            "train loss:2.2047405116263703\n",
            "train loss:2.1446660653870007\n",
            "train loss:2.129170741607608\n",
            "=== epoch:90, train acc:0.3466666666666667, test acc:0.2793 ===\n",
            "train loss:2.1616916651754288\n",
            "train loss:2.167137959842052\n",
            "train loss:2.154789716715011\n",
            "=== epoch:91, train acc:0.35333333333333333, test acc:0.2818 ===\n",
            "train loss:2.1726999227104096\n",
            "train loss:2.182089542618571\n",
            "train loss:2.1604772280032143\n",
            "=== epoch:92, train acc:0.35, test acc:0.2795 ===\n",
            "train loss:2.1440867628762117\n",
            "train loss:2.1093103028421263\n",
            "train loss:2.143235204986025\n",
            "=== epoch:93, train acc:0.3433333333333333, test acc:0.2739 ===\n",
            "train loss:2.161129190203724\n",
            "train loss:2.1712908674831746\n",
            "train loss:2.1832532344174673\n",
            "=== epoch:94, train acc:0.36333333333333334, test acc:0.2879 ===\n",
            "train loss:2.1875409904207475\n",
            "train loss:2.147209381258507\n",
            "train loss:2.1812275976433524\n",
            "=== epoch:95, train acc:0.37, test acc:0.2904 ===\n",
            "train loss:2.160637746268805\n",
            "train loss:2.1882637326810093\n",
            "train loss:2.166621953239638\n",
            "=== epoch:96, train acc:0.36333333333333334, test acc:0.2905 ===\n",
            "train loss:2.148529337459691\n",
            "train loss:2.1656410242210358\n",
            "train loss:2.1437616339223906\n",
            "=== epoch:97, train acc:0.36666666666666664, test acc:0.2946 ===\n",
            "train loss:2.1941272650097763\n",
            "train loss:2.1652666263223814\n",
            "train loss:2.1222817288271125\n",
            "=== epoch:98, train acc:0.37, test acc:0.3035 ===\n",
            "train loss:2.142805566189823\n",
            "train loss:2.1394301869545878\n",
            "train loss:2.1640141043990586\n",
            "=== epoch:99, train acc:0.36666666666666664, test acc:0.3028 ===\n",
            "train loss:2.163454511460233\n",
            "train loss:2.122593576328754\n",
            "train loss:2.1090238085089634\n",
            "=== epoch:100, train acc:0.36666666666666664, test acc:0.3041 ===\n",
            "train loss:2.12234267370295\n",
            "train loss:2.107198682518569\n",
            "train loss:2.177764861304316\n",
            "=== epoch:101, train acc:0.36666666666666664, test acc:0.2989 ===\n",
            "train loss:2.1536176653945263\n",
            "train loss:2.146256346537899\n",
            "train loss:2.1374723870721333\n",
            "=== epoch:102, train acc:0.36666666666666664, test acc:0.2999 ===\n",
            "train loss:2.1536274100189665\n",
            "train loss:2.1569233104011705\n",
            "train loss:2.1539898645894313\n",
            "=== epoch:103, train acc:0.38, test acc:0.3096 ===\n",
            "train loss:2.1167210107520473\n",
            "train loss:2.152582264115399\n",
            "train loss:2.105564110220297\n",
            "=== epoch:104, train acc:0.3933333333333333, test acc:0.3176 ===\n",
            "train loss:2.1337645378753582\n",
            "train loss:2.147404973579059\n",
            "train loss:2.1662074400598583\n",
            "=== epoch:105, train acc:0.4033333333333333, test acc:0.3205 ===\n",
            "train loss:2.151476264804676\n",
            "train loss:2.11651592792487\n",
            "train loss:2.113507944240703\n",
            "=== epoch:106, train acc:0.4033333333333333, test acc:0.3207 ===\n",
            "train loss:2.103837794693897\n",
            "train loss:2.114219952725173\n",
            "train loss:2.1736089389897124\n",
            "=== epoch:107, train acc:0.4166666666666667, test acc:0.3277 ===\n",
            "train loss:2.1021790611009625\n",
            "train loss:2.1418922173080954\n",
            "train loss:2.138900296621892\n",
            "=== epoch:108, train acc:0.4166666666666667, test acc:0.3292 ===\n",
            "train loss:2.1376201224644187\n",
            "train loss:2.1035070386914287\n",
            "train loss:2.1482790587743086\n",
            "=== epoch:109, train acc:0.42, test acc:0.336 ===\n",
            "train loss:2.1396430123042838\n",
            "train loss:2.1037259719978603\n",
            "train loss:2.120010308320203\n",
            "=== epoch:110, train acc:0.4266666666666667, test acc:0.3353 ===\n",
            "train loss:2.1407084463147354\n",
            "train loss:2.117392335645736\n",
            "train loss:2.1822947655119473\n",
            "=== epoch:111, train acc:0.4266666666666667, test acc:0.3374 ===\n",
            "train loss:2.1116055784596295\n",
            "train loss:2.11840804978457\n",
            "train loss:2.113696756586458\n",
            "=== epoch:112, train acc:0.44333333333333336, test acc:0.3462 ===\n",
            "train loss:2.1107087969368807\n",
            "train loss:2.169273890370345\n",
            "train loss:2.112924601569075\n",
            "=== epoch:113, train acc:0.44666666666666666, test acc:0.3468 ===\n",
            "train loss:2.065447177726496\n",
            "train loss:2.0705075395781436\n",
            "train loss:2.1142372257103337\n",
            "=== epoch:114, train acc:0.44, test acc:0.3424 ===\n",
            "train loss:2.0976765526444265\n",
            "train loss:2.1296535228587343\n",
            "train loss:2.1427945720828565\n",
            "=== epoch:115, train acc:0.45666666666666667, test acc:0.3516 ===\n",
            "train loss:2.0997302210247715\n",
            "train loss:2.1134203461420404\n",
            "train loss:2.164991480154445\n",
            "=== epoch:116, train acc:0.4666666666666667, test acc:0.3584 ===\n",
            "train loss:2.091695288376048\n",
            "train loss:2.088299635818762\n",
            "train loss:2.130232932251283\n",
            "=== epoch:117, train acc:0.47333333333333333, test acc:0.3684 ===\n",
            "train loss:2.148860069086359\n",
            "train loss:2.050345344115822\n",
            "train loss:2.1001140665440383\n",
            "=== epoch:118, train acc:0.48, test acc:0.3715 ===\n",
            "train loss:2.0855237206292307\n",
            "train loss:2.080958077543643\n",
            "train loss:2.0539978963792622\n",
            "=== epoch:119, train acc:0.47333333333333333, test acc:0.364 ===\n",
            "train loss:2.0784825896300534\n",
            "train loss:2.093888367437759\n",
            "train loss:2.086624029436768\n",
            "=== epoch:120, train acc:0.47333333333333333, test acc:0.3619 ===\n",
            "train loss:2.0577623528754514\n",
            "train loss:2.124362116444351\n",
            "train loss:2.0944205198114174\n",
            "=== epoch:121, train acc:0.48, test acc:0.3627 ===\n",
            "train loss:2.0760910988619328\n",
            "train loss:2.0469309409769436\n",
            "train loss:2.0645603481701342\n",
            "=== epoch:122, train acc:0.47333333333333333, test acc:0.3615 ===\n",
            "train loss:2.04615753964349\n",
            "train loss:2.0764208408675024\n",
            "train loss:2.06394761342093\n",
            "=== epoch:123, train acc:0.47333333333333333, test acc:0.3615 ===\n",
            "train loss:2.082073623818668\n",
            "train loss:2.1334687492225677\n",
            "train loss:2.0363840570747795\n",
            "=== epoch:124, train acc:0.47333333333333333, test acc:0.3657 ===\n",
            "train loss:2.05158738269847\n",
            "train loss:2.072511636444224\n",
            "train loss:2.0700970646973067\n",
            "=== epoch:125, train acc:0.4766666666666667, test acc:0.3694 ===\n",
            "train loss:2.097888837567801\n",
            "train loss:2.063978980608034\n",
            "train loss:2.0714180983241652\n",
            "=== epoch:126, train acc:0.4866666666666667, test acc:0.3767 ===\n",
            "train loss:2.075175940820246\n",
            "train loss:2.114370912860593\n",
            "train loss:2.0736916366706555\n",
            "=== epoch:127, train acc:0.4866666666666667, test acc:0.3841 ===\n",
            "train loss:2.0758755149737085\n",
            "train loss:2.0352908178407474\n",
            "train loss:2.064833921923343\n",
            "=== epoch:128, train acc:0.49, test acc:0.3814 ===\n",
            "train loss:2.0392787957525864\n",
            "train loss:2.1246014346924804\n",
            "train loss:2.080685322912217\n",
            "=== epoch:129, train acc:0.49666666666666665, test acc:0.3859 ===\n",
            "train loss:2.028377679632518\n",
            "train loss:2.0115531893923655\n",
            "train loss:2.0875022844209057\n",
            "=== epoch:130, train acc:0.49333333333333335, test acc:0.3858 ===\n",
            "train loss:2.094173374759293\n",
            "train loss:2.0430911608808797\n",
            "train loss:2.018232301698223\n",
            "=== epoch:131, train acc:0.49, test acc:0.3832 ===\n",
            "train loss:2.0583088756279064\n",
            "train loss:2.0508520361457196\n",
            "train loss:2.081628253332775\n",
            "=== epoch:132, train acc:0.4866666666666667, test acc:0.3862 ===\n",
            "train loss:2.0859271091745333\n",
            "train loss:2.067183658177688\n",
            "train loss:2.059613964531314\n",
            "=== epoch:133, train acc:0.49666666666666665, test acc:0.3962 ===\n",
            "train loss:2.0425522639278153\n",
            "train loss:2.0253271718805763\n",
            "train loss:2.070453418448313\n",
            "=== epoch:134, train acc:0.49666666666666665, test acc:0.3986 ===\n",
            "train loss:2.0142499395125073\n",
            "train loss:2.0947540162103784\n",
            "train loss:2.0545668371808765\n",
            "=== epoch:135, train acc:0.5, test acc:0.4032 ===\n",
            "train loss:1.9975054064579882\n",
            "train loss:2.0080282847230784\n",
            "train loss:2.064361737447154\n",
            "=== epoch:136, train acc:0.49333333333333335, test acc:0.4074 ===\n",
            "train loss:2.065600273426175\n",
            "train loss:2.0214036859070417\n",
            "train loss:2.0749742236787942\n",
            "=== epoch:137, train acc:0.49666666666666665, test acc:0.4084 ===\n",
            "train loss:2.0948220352687925\n",
            "train loss:2.0726073944872994\n",
            "train loss:1.9936139923178304\n",
            "=== epoch:138, train acc:0.5066666666666667, test acc:0.4104 ===\n",
            "train loss:2.0208599249030765\n",
            "train loss:2.019671868028105\n",
            "train loss:1.9647768275570057\n",
            "=== epoch:139, train acc:0.5066666666666667, test acc:0.4102 ===\n",
            "train loss:2.0095587695151926\n",
            "train loss:2.087575753604627\n",
            "train loss:1.9993437565966072\n",
            "=== epoch:140, train acc:0.5033333333333333, test acc:0.407 ===\n",
            "train loss:2.0382849405545143\n",
            "train loss:2.019633832401931\n",
            "train loss:2.0455588999870207\n",
            "=== epoch:141, train acc:0.51, test acc:0.4096 ===\n",
            "train loss:2.0368917112863243\n",
            "train loss:2.00528300797026\n",
            "train loss:2.0601693594378805\n",
            "=== epoch:142, train acc:0.5133333333333333, test acc:0.4151 ===\n",
            "train loss:2.0374631550805664\n",
            "train loss:2.073759990898571\n",
            "train loss:2.028403443635979\n",
            "=== epoch:143, train acc:0.52, test acc:0.4175 ===\n",
            "train loss:1.9565894158237083\n",
            "train loss:1.959814674307907\n",
            "train loss:2.0683043927319456\n",
            "=== epoch:144, train acc:0.52, test acc:0.4178 ===\n",
            "train loss:2.0308930767696713\n",
            "train loss:2.0174369323729446\n",
            "train loss:2.0025411148328747\n",
            "=== epoch:145, train acc:0.5133333333333333, test acc:0.4217 ===\n",
            "train loss:1.9661573932324992\n",
            "train loss:1.9901923135866262\n",
            "train loss:1.9352257910849624\n",
            "=== epoch:146, train acc:0.5, test acc:0.4194 ===\n",
            "train loss:2.025875366347033\n",
            "train loss:2.0509083154311636\n",
            "train loss:2.035057736930623\n",
            "=== epoch:147, train acc:0.5033333333333333, test acc:0.4247 ===\n",
            "train loss:2.003807458572625\n",
            "train loss:2.0067500382224774\n",
            "train loss:2.01450190926348\n",
            "=== epoch:148, train acc:0.49666666666666665, test acc:0.4238 ===\n",
            "train loss:1.9979128338739391\n",
            "train loss:1.9243495091712544\n",
            "train loss:1.9881239010767693\n",
            "=== epoch:149, train acc:0.49666666666666665, test acc:0.4208 ===\n",
            "train loss:2.0043661285976055\n",
            "train loss:1.9974695839004772\n",
            "train loss:1.953508361154652\n",
            "=== epoch:150, train acc:0.49666666666666665, test acc:0.4214 ===\n",
            "train loss:2.046730247976926\n",
            "train loss:1.8821772369673226\n",
            "train loss:2.001120400846079\n",
            "=== epoch:151, train acc:0.49, test acc:0.4217 ===\n",
            "train loss:1.991412719461084\n",
            "train loss:2.0002849224197417\n",
            "train loss:1.981491820644012\n",
            "=== epoch:152, train acc:0.4866666666666667, test acc:0.4227 ===\n",
            "train loss:1.9725439889341732\n",
            "train loss:1.8612781458034167\n",
            "train loss:1.9699462318002718\n",
            "=== epoch:153, train acc:0.4866666666666667, test acc:0.4205 ===\n",
            "train loss:1.9973038252383999\n",
            "train loss:1.9980516575385885\n",
            "train loss:2.0106835729400774\n",
            "=== epoch:154, train acc:0.49666666666666665, test acc:0.4227 ===\n",
            "train loss:1.966519885538712\n",
            "train loss:1.9316368071722607\n",
            "train loss:1.873710003472666\n",
            "=== epoch:155, train acc:0.49666666666666665, test acc:0.4227 ===\n",
            "train loss:2.0043400025655917\n",
            "train loss:1.8477479119536602\n",
            "train loss:2.008980428583428\n",
            "=== epoch:156, train acc:0.49666666666666665, test acc:0.4215 ===\n",
            "train loss:1.9062166507243197\n",
            "train loss:2.002357828281384\n",
            "train loss:1.9178139172464574\n",
            "=== epoch:157, train acc:0.49666666666666665, test acc:0.4236 ===\n",
            "train loss:1.9784034951890135\n",
            "train loss:2.004346401680733\n",
            "train loss:1.8996175984520824\n",
            "=== epoch:158, train acc:0.49666666666666665, test acc:0.4229 ===\n",
            "train loss:1.9307030989709764\n",
            "train loss:1.92067492735366\n",
            "train loss:1.974628496388532\n",
            "=== epoch:159, train acc:0.49666666666666665, test acc:0.4232 ===\n",
            "train loss:1.994103183077526\n",
            "train loss:2.0111628410685203\n",
            "train loss:1.970073396147146\n",
            "=== epoch:160, train acc:0.49666666666666665, test acc:0.4253 ===\n",
            "train loss:1.793975023307285\n",
            "train loss:1.9817383936491373\n",
            "train loss:1.9590864479202759\n",
            "=== epoch:161, train acc:0.5, test acc:0.4292 ===\n",
            "train loss:2.0534367050943394\n",
            "train loss:1.898120102073265\n",
            "train loss:1.8240397592048754\n",
            "=== epoch:162, train acc:0.5, test acc:0.4284 ===\n",
            "train loss:1.8749657573262382\n",
            "train loss:1.9157353064191307\n",
            "train loss:1.955973203507385\n",
            "=== epoch:163, train acc:0.5066666666666667, test acc:0.4285 ===\n",
            "train loss:1.922603393668902\n",
            "train loss:1.931801582200278\n",
            "train loss:1.844721208961173\n",
            "=== epoch:164, train acc:0.5033333333333333, test acc:0.4261 ===\n",
            "train loss:1.9779732395952283\n",
            "train loss:1.9592808459214124\n",
            "train loss:1.9778374414115105\n",
            "=== epoch:165, train acc:0.5066666666666667, test acc:0.4313 ===\n",
            "train loss:1.8356918021421165\n",
            "train loss:1.9134796505699896\n",
            "train loss:1.899618292963877\n",
            "=== epoch:166, train acc:0.51, test acc:0.4338 ===\n",
            "train loss:1.9361716964539868\n",
            "train loss:1.8666141034201615\n",
            "train loss:1.901574338659644\n",
            "=== epoch:167, train acc:0.51, test acc:0.4343 ===\n",
            "train loss:1.9438802750801782\n",
            "train loss:1.8348587517899193\n",
            "train loss:1.9341601259960168\n",
            "=== epoch:168, train acc:0.5133333333333333, test acc:0.4371 ===\n",
            "train loss:1.9908327914819768\n",
            "train loss:1.7840200847904653\n",
            "train loss:1.961245349990367\n",
            "=== epoch:169, train acc:0.5133333333333333, test acc:0.436 ===\n",
            "train loss:1.9437493528533951\n",
            "train loss:1.8692247331800937\n",
            "train loss:1.9508012386076756\n",
            "=== epoch:170, train acc:0.51, test acc:0.4373 ===\n",
            "train loss:1.8124678040868918\n",
            "train loss:1.8605963145096964\n",
            "train loss:1.898055928152305\n",
            "=== epoch:171, train acc:0.51, test acc:0.4366 ===\n",
            "train loss:1.914179782696157\n",
            "train loss:1.8175167829553351\n",
            "train loss:1.8198730388657172\n",
            "=== epoch:172, train acc:0.5133333333333333, test acc:0.4362 ===\n",
            "train loss:1.870548639325483\n",
            "train loss:1.9219811769717061\n",
            "train loss:1.8623157827133405\n",
            "=== epoch:173, train acc:0.5066666666666667, test acc:0.4363 ===\n",
            "train loss:1.9012390218436266\n",
            "train loss:1.7954016903882144\n",
            "train loss:1.9245908265093907\n",
            "=== epoch:174, train acc:0.5033333333333333, test acc:0.4368 ===\n",
            "train loss:1.8423994704306441\n",
            "train loss:1.86687780837801\n",
            "train loss:1.8780194182104288\n",
            "=== epoch:175, train acc:0.5033333333333333, test acc:0.4396 ===\n",
            "train loss:1.8807281735501513\n",
            "train loss:1.8122423814324822\n",
            "train loss:1.7948677280345393\n",
            "=== epoch:176, train acc:0.5033333333333333, test acc:0.4383 ===\n",
            "train loss:1.9633805901058172\n",
            "train loss:1.822263393525627\n",
            "train loss:1.8671649258967344\n",
            "=== epoch:177, train acc:0.5033333333333333, test acc:0.4381 ===\n",
            "train loss:1.8815664158976342\n",
            "train loss:1.837692940733772\n",
            "train loss:1.9214305611234852\n",
            "=== epoch:178, train acc:0.5066666666666667, test acc:0.4401 ===\n",
            "train loss:1.8030261336067064\n",
            "train loss:1.6729202760694446\n",
            "train loss:1.8611485913398196\n",
            "=== epoch:179, train acc:0.51, test acc:0.4385 ===\n",
            "train loss:1.7794745081228243\n",
            "train loss:1.7085038276855058\n",
            "train loss:1.8044022273531746\n",
            "=== epoch:180, train acc:0.51, test acc:0.4384 ===\n",
            "train loss:1.9506617867998872\n",
            "train loss:1.8659668396937608\n",
            "train loss:1.911915276098624\n",
            "=== epoch:181, train acc:0.5133333333333333, test acc:0.4414 ===\n",
            "train loss:1.9160141617667827\n",
            "train loss:1.8066500314376122\n",
            "train loss:1.784105860003234\n",
            "=== epoch:182, train acc:0.51, test acc:0.4402 ===\n",
            "train loss:1.8436243468370896\n",
            "train loss:1.830218947090472\n",
            "train loss:1.9379312359325522\n",
            "=== epoch:183, train acc:0.5166666666666667, test acc:0.4414 ===\n",
            "train loss:1.7787087312783538\n",
            "train loss:1.7375150313476917\n",
            "train loss:1.8818698015365514\n",
            "=== epoch:184, train acc:0.52, test acc:0.444 ===\n",
            "train loss:1.937010750462922\n",
            "train loss:1.8109223467925781\n",
            "train loss:1.7728660375589462\n",
            "=== epoch:185, train acc:0.5233333333333333, test acc:0.444 ===\n",
            "train loss:1.7936799186336\n",
            "train loss:1.7970580843609683\n",
            "train loss:1.7957869957018613\n",
            "=== epoch:186, train acc:0.5233333333333333, test acc:0.4454 ===\n",
            "train loss:1.8313296214760055\n",
            "train loss:1.8040608113249317\n",
            "train loss:1.8212172165500018\n",
            "=== epoch:187, train acc:0.5166666666666667, test acc:0.4474 ===\n",
            "train loss:1.8339738962446845\n",
            "train loss:1.8539359887253992\n",
            "train loss:1.8344200005869609\n",
            "=== epoch:188, train acc:0.5233333333333333, test acc:0.4461 ===\n",
            "train loss:1.825788338707498\n",
            "train loss:1.7703238504449024\n",
            "train loss:1.9043065550533578\n",
            "=== epoch:189, train acc:0.5233333333333333, test acc:0.4493 ===\n",
            "train loss:1.744350720804198\n",
            "train loss:1.8219758640106374\n",
            "train loss:1.829581455569716\n",
            "=== epoch:190, train acc:0.5266666666666666, test acc:0.4491 ===\n",
            "train loss:1.7620151419108954\n",
            "train loss:1.6415813119473526\n",
            "train loss:1.6995791754554745\n",
            "=== epoch:191, train acc:0.5233333333333333, test acc:0.4463 ===\n",
            "train loss:1.8288557367331688\n",
            "train loss:1.7609204541023533\n",
            "train loss:1.8093391301477633\n",
            "=== epoch:192, train acc:0.52, test acc:0.4498 ===\n",
            "train loss:1.7538927176886394\n",
            "train loss:1.799677152752332\n",
            "train loss:1.8032209221497768\n",
            "=== epoch:193, train acc:0.5233333333333333, test acc:0.4503 ===\n",
            "train loss:1.758582066818637\n",
            "train loss:1.9117169528564881\n",
            "train loss:1.8059839620529274\n",
            "=== epoch:194, train acc:0.52, test acc:0.4515 ===\n",
            "train loss:1.6762140330440207\n",
            "train loss:1.7296988457420313\n",
            "train loss:1.7017571439156765\n",
            "=== epoch:195, train acc:0.5166666666666667, test acc:0.452 ===\n",
            "train loss:1.7626973599132894\n",
            "train loss:1.713304299943921\n",
            "train loss:1.6540866752335404\n",
            "=== epoch:196, train acc:0.5166666666666667, test acc:0.4531 ===\n",
            "train loss:1.8202256498979592\n",
            "train loss:1.8254535138732093\n",
            "train loss:1.8108306477466285\n",
            "=== epoch:197, train acc:0.5166666666666667, test acc:0.454 ===\n",
            "train loss:1.7475140603794352\n",
            "train loss:1.8697604601062157\n",
            "train loss:1.7633680866684882\n",
            "=== epoch:198, train acc:0.53, test acc:0.4592 ===\n",
            "train loss:1.7216474068805818\n",
            "train loss:1.821991099173556\n",
            "train loss:1.7970117541318478\n",
            "=== epoch:199, train acc:0.5333333333333333, test acc:0.4604 ===\n",
            "train loss:1.7135409174937692\n",
            "train loss:1.6642684858207377\n",
            "train loss:1.7661778040629494\n",
            "=== epoch:200, train acc:0.53, test acc:0.4601 ===\n",
            "train loss:1.7670043879565274\n",
            "train loss:1.6637321031314578\n",
            "train loss:1.7618931028539786\n",
            "=== epoch:201, train acc:0.5333333333333333, test acc:0.4616 ===\n",
            "train loss:1.796254513058285\n",
            "train loss:1.739749565065878\n",
            "train loss:1.7631873173201331\n",
            "=== epoch:202, train acc:0.5266666666666666, test acc:0.4599 ===\n",
            "train loss:1.642370873967206\n",
            "train loss:1.7813825715356437\n",
            "train loss:1.8193209800621855\n",
            "=== epoch:203, train acc:0.5266666666666666, test acc:0.4615 ===\n",
            "train loss:1.742839055031682\n",
            "train loss:1.7513610443341179\n",
            "train loss:1.732870855157148\n",
            "=== epoch:204, train acc:0.5266666666666666, test acc:0.4625 ===\n",
            "train loss:1.6724321142558816\n",
            "train loss:1.642588331130764\n",
            "train loss:1.6372198036581649\n",
            "=== epoch:205, train acc:0.5266666666666666, test acc:0.4621 ===\n",
            "train loss:1.6258980219074073\n",
            "train loss:1.5927040808454764\n",
            "train loss:1.653142201542113\n",
            "=== epoch:206, train acc:0.5333333333333333, test acc:0.4638 ===\n",
            "train loss:1.7496849549412699\n",
            "train loss:1.6794655823892974\n",
            "train loss:1.6013347658474513\n",
            "=== epoch:207, train acc:0.5333333333333333, test acc:0.4648 ===\n",
            "train loss:1.7922974052251366\n",
            "train loss:1.6706241219879123\n",
            "train loss:1.6623817247339374\n",
            "=== epoch:208, train acc:0.5333333333333333, test acc:0.465 ===\n",
            "train loss:1.6628955983910916\n",
            "train loss:1.7890440181248035\n",
            "train loss:1.7041843350612806\n",
            "=== epoch:209, train acc:0.5333333333333333, test acc:0.4651 ===\n",
            "train loss:1.6417259202659125\n",
            "train loss:1.6388030818156887\n",
            "train loss:1.730343940699116\n",
            "=== epoch:210, train acc:0.5333333333333333, test acc:0.4655 ===\n",
            "train loss:1.7845126700265608\n",
            "train loss:1.7281985565143068\n",
            "train loss:1.7279987854565035\n",
            "=== epoch:211, train acc:0.5333333333333333, test acc:0.4663 ===\n",
            "train loss:1.7224922911973588\n",
            "train loss:1.6349513637439532\n",
            "train loss:1.7631875312275165\n",
            "=== epoch:212, train acc:0.5366666666666666, test acc:0.4674 ===\n",
            "train loss:1.6518439873996411\n",
            "train loss:1.6558387460764132\n",
            "train loss:1.5884883938084018\n",
            "=== epoch:213, train acc:0.5366666666666666, test acc:0.4661 ===\n",
            "train loss:1.6635991054162298\n",
            "train loss:1.6424351708683411\n",
            "train loss:1.6430236016686062\n",
            "=== epoch:214, train acc:0.5366666666666666, test acc:0.4656 ===\n",
            "train loss:1.7230213702811823\n",
            "train loss:1.6733735802318754\n",
            "train loss:1.6513077816390025\n",
            "=== epoch:215, train acc:0.5333333333333333, test acc:0.4669 ===\n",
            "train loss:1.6943508477520397\n",
            "train loss:1.5895871655525589\n",
            "train loss:1.738638622899255\n",
            "=== epoch:216, train acc:0.5333333333333333, test acc:0.4676 ===\n",
            "train loss:1.6324086449211959\n",
            "train loss:1.5790037565814157\n",
            "train loss:1.5762049916534009\n",
            "=== epoch:217, train acc:0.53, test acc:0.4654 ===\n",
            "train loss:1.7089086201890984\n",
            "train loss:1.8403608363165214\n",
            "train loss:1.5800981385559905\n",
            "=== epoch:218, train acc:0.5366666666666666, test acc:0.4652 ===\n",
            "train loss:1.610295946528452\n",
            "train loss:1.6445290817165386\n",
            "train loss:1.7164430761691452\n",
            "=== epoch:219, train acc:0.5333333333333333, test acc:0.4671 ===\n",
            "train loss:1.6260368759349182\n",
            "train loss:1.6373396312497022\n",
            "train loss:1.6601461801442474\n",
            "=== epoch:220, train acc:0.5333333333333333, test acc:0.4666 ===\n",
            "train loss:1.6911562008718042\n",
            "train loss:1.7131666714216458\n",
            "train loss:1.5405282012866044\n",
            "=== epoch:221, train acc:0.5333333333333333, test acc:0.4698 ===\n",
            "train loss:1.7110747886719537\n",
            "train loss:1.651915147903367\n",
            "train loss:1.5883655476624283\n",
            "=== epoch:222, train acc:0.5366666666666666, test acc:0.4699 ===\n",
            "train loss:1.7475720345429124\n",
            "train loss:1.671749441516497\n",
            "train loss:1.6279903672227933\n",
            "=== epoch:223, train acc:0.5333333333333333, test acc:0.4698 ===\n",
            "train loss:1.7083707611036985\n",
            "train loss:1.642532576184081\n",
            "train loss:1.714097380331025\n",
            "=== epoch:224, train acc:0.5333333333333333, test acc:0.4692 ===\n",
            "train loss:1.5333815059875766\n",
            "train loss:1.5915276662530167\n",
            "train loss:1.6601792400485056\n",
            "=== epoch:225, train acc:0.5366666666666666, test acc:0.4692 ===\n",
            "train loss:1.7657464367208584\n",
            "train loss:1.6168645130031039\n",
            "train loss:1.5435872287627164\n",
            "=== epoch:226, train acc:0.5366666666666666, test acc:0.4692 ===\n",
            "train loss:1.6179816053776517\n",
            "train loss:1.5734463224139168\n",
            "train loss:1.478586141416099\n",
            "=== epoch:227, train acc:0.5366666666666666, test acc:0.47 ===\n",
            "train loss:1.543248150523205\n",
            "train loss:1.6678115400122184\n",
            "train loss:1.5296728096349788\n",
            "=== epoch:228, train acc:0.5366666666666666, test acc:0.4704 ===\n",
            "train loss:1.6548951781533774\n",
            "train loss:1.7099322892294735\n",
            "train loss:1.5561376729269034\n",
            "=== epoch:229, train acc:0.5366666666666666, test acc:0.4705 ===\n",
            "train loss:1.5862324385260935\n",
            "train loss:1.7478026267254736\n",
            "train loss:1.555391083159088\n",
            "=== epoch:230, train acc:0.5366666666666666, test acc:0.4711 ===\n",
            "train loss:1.4849233488084708\n",
            "train loss:1.3572940134041773\n",
            "train loss:1.675469349743432\n",
            "=== epoch:231, train acc:0.5366666666666666, test acc:0.4714 ===\n",
            "train loss:1.566016248230166\n",
            "train loss:1.610149675449302\n",
            "train loss:1.4081737580009388\n",
            "=== epoch:232, train acc:0.5366666666666666, test acc:0.4726 ===\n",
            "train loss:1.5660168349365795\n",
            "train loss:1.4323431499286015\n",
            "train loss:1.6487247124461029\n",
            "=== epoch:233, train acc:0.5333333333333333, test acc:0.4713 ===\n",
            "train loss:1.5221909793736526\n",
            "train loss:1.596304966445972\n",
            "train loss:1.5555117276230976\n",
            "=== epoch:234, train acc:0.5333333333333333, test acc:0.4707 ===\n",
            "train loss:1.4701120168613908\n",
            "train loss:1.5418379814806704\n",
            "train loss:1.5913397443937456\n",
            "=== epoch:235, train acc:0.5333333333333333, test acc:0.4713 ===\n",
            "train loss:1.523469964841471\n",
            "train loss:1.6535380140689424\n",
            "train loss:1.492209643230243\n",
            "=== epoch:236, train acc:0.5366666666666666, test acc:0.4718 ===\n",
            "train loss:1.5302282071236848\n",
            "train loss:1.4537105463743711\n",
            "train loss:1.4659749133413014\n",
            "=== epoch:237, train acc:0.5366666666666666, test acc:0.473 ===\n",
            "train loss:1.4820719032216465\n",
            "train loss:1.5961003453953901\n",
            "train loss:1.6120084528478922\n",
            "=== epoch:238, train acc:0.5366666666666666, test acc:0.4745 ===\n",
            "train loss:1.437244226297125\n",
            "train loss:1.5293465143553997\n",
            "train loss:1.5069404152249664\n",
            "=== epoch:239, train acc:0.5366666666666666, test acc:0.4752 ===\n",
            "train loss:1.5998174375269787\n",
            "train loss:1.593774209001627\n",
            "train loss:1.4597642358525804\n",
            "=== epoch:240, train acc:0.5366666666666666, test acc:0.4739 ===\n",
            "train loss:1.7241932129757254\n",
            "train loss:1.4420411325324727\n",
            "train loss:1.4599194719564421\n",
            "=== epoch:241, train acc:0.5366666666666666, test acc:0.4746 ===\n",
            "train loss:1.493828160874379\n",
            "train loss:1.5063611274881188\n",
            "train loss:1.4726934421687248\n",
            "=== epoch:242, train acc:0.5366666666666666, test acc:0.4749 ===\n",
            "train loss:1.5787450036712776\n",
            "train loss:1.4937160542032493\n",
            "train loss:1.4141191654022647\n",
            "=== epoch:243, train acc:0.5366666666666666, test acc:0.4767 ===\n",
            "train loss:1.5450140936597296\n",
            "train loss:1.516410838304357\n",
            "train loss:1.5478470376727196\n",
            "=== epoch:244, train acc:0.5366666666666666, test acc:0.4754 ===\n",
            "train loss:1.5975424134802458\n",
            "train loss:1.4588406627693835\n",
            "train loss:1.3709185311931065\n",
            "=== epoch:245, train acc:0.5366666666666666, test acc:0.4734 ===\n",
            "train loss:1.5647580222917514\n",
            "train loss:1.5446850841002842\n",
            "train loss:1.5625173674303319\n",
            "=== epoch:246, train acc:0.5366666666666666, test acc:0.4752 ===\n",
            "train loss:1.4495319761755345\n",
            "train loss:1.5205466125337255\n",
            "train loss:1.497839492321937\n",
            "=== epoch:247, train acc:0.5366666666666666, test acc:0.4755 ===\n",
            "train loss:1.5217893712495836\n",
            "train loss:1.4829287148586952\n",
            "train loss:1.537481209164171\n",
            "=== epoch:248, train acc:0.5366666666666666, test acc:0.4745 ===\n",
            "train loss:1.4792016113896616\n",
            "train loss:1.576824865912562\n",
            "train loss:1.556353993570509\n",
            "=== epoch:249, train acc:0.5333333333333333, test acc:0.4736 ===\n",
            "train loss:1.3876919616434895\n",
            "train loss:1.3832961968605055\n",
            "train loss:1.4517070692719658\n",
            "=== epoch:250, train acc:0.5333333333333333, test acc:0.4745 ===\n",
            "train loss:1.601863136244401\n",
            "train loss:1.4659322381440185\n",
            "train loss:1.538512330462918\n",
            "=== epoch:251, train acc:0.54, test acc:0.4773 ===\n",
            "train loss:1.5106738133347093\n",
            "train loss:1.4671617329715003\n",
            "train loss:1.5561776560936875\n",
            "=== epoch:252, train acc:0.54, test acc:0.4784 ===\n",
            "train loss:1.4508088403342279\n",
            "train loss:1.5122748615604613\n",
            "train loss:1.4051617555070732\n",
            "=== epoch:253, train acc:0.5366666666666666, test acc:0.4798 ===\n",
            "train loss:1.295578234059518\n",
            "train loss:1.3684536253154531\n",
            "train loss:1.5539231174212205\n",
            "=== epoch:254, train acc:0.5366666666666666, test acc:0.4792 ===\n",
            "train loss:1.331216539320051\n",
            "train loss:1.4321024529554722\n",
            "train loss:1.4651080926203517\n",
            "=== epoch:255, train acc:0.54, test acc:0.4796 ===\n",
            "train loss:1.5054286007237354\n",
            "train loss:1.3113104843506886\n",
            "train loss:1.4678967199369684\n",
            "=== epoch:256, train acc:0.54, test acc:0.4803 ===\n",
            "train loss:1.5945686060201678\n",
            "train loss:1.4087854672910933\n",
            "train loss:1.4060631979292912\n",
            "=== epoch:257, train acc:0.54, test acc:0.4823 ===\n",
            "train loss:1.4721528727913082\n",
            "train loss:1.586997148110192\n",
            "train loss:1.376058222904439\n",
            "=== epoch:258, train acc:0.5433333333333333, test acc:0.4803 ===\n",
            "train loss:1.5511544172527585\n",
            "train loss:1.4333909603997594\n",
            "train loss:1.5517526352868523\n",
            "=== epoch:259, train acc:0.54, test acc:0.4805 ===\n",
            "train loss:1.4934636250883029\n",
            "train loss:1.4842494764232403\n",
            "train loss:1.5149485550281978\n",
            "=== epoch:260, train acc:0.54, test acc:0.4803 ===\n",
            "train loss:1.3782358972322697\n",
            "train loss:1.2703365156828337\n",
            "train loss:1.3838487689348673\n",
            "=== epoch:261, train acc:0.5433333333333333, test acc:0.4834 ===\n",
            "train loss:1.3985076938815402\n",
            "train loss:1.32848096656107\n",
            "train loss:1.4890446976313505\n",
            "=== epoch:262, train acc:0.54, test acc:0.4827 ===\n",
            "train loss:1.4747436053902914\n",
            "train loss:1.36043154497916\n",
            "train loss:1.4421363197120634\n",
            "=== epoch:263, train acc:0.54, test acc:0.4833 ===\n",
            "train loss:1.4730400303690068\n",
            "train loss:1.6160430338665521\n",
            "train loss:1.4849638160074528\n",
            "=== epoch:264, train acc:0.5466666666666666, test acc:0.4868 ===\n",
            "train loss:1.4622174469609126\n",
            "train loss:1.4331294967570423\n",
            "train loss:1.3695402337993576\n",
            "=== epoch:265, train acc:0.5433333333333333, test acc:0.4866 ===\n",
            "train loss:1.4673955835702293\n",
            "train loss:1.4561128852874448\n",
            "train loss:1.342302558502919\n",
            "=== epoch:266, train acc:0.55, test acc:0.4868 ===\n",
            "train loss:1.4231043675539832\n",
            "train loss:1.5269540621924793\n",
            "train loss:1.4918179717662239\n",
            "=== epoch:267, train acc:0.55, test acc:0.4862 ===\n",
            "train loss:1.4355929434376333\n",
            "train loss:1.4931438928200154\n",
            "train loss:1.4704689028171678\n",
            "=== epoch:268, train acc:0.5466666666666666, test acc:0.4856 ===\n",
            "train loss:1.4432656903893275\n",
            "train loss:1.3274942434999675\n",
            "train loss:1.4975752597600427\n",
            "=== epoch:269, train acc:0.5466666666666666, test acc:0.4849 ===\n",
            "train loss:1.3226845832907101\n",
            "train loss:1.496498653630296\n",
            "train loss:1.4387360809575207\n",
            "=== epoch:270, train acc:0.5466666666666666, test acc:0.4864 ===\n",
            "train loss:1.270652553780909\n",
            "train loss:1.3791878332622998\n",
            "train loss:1.4226461069612282\n",
            "=== epoch:271, train acc:0.5466666666666666, test acc:0.4862 ===\n",
            "train loss:1.3717302884475677\n",
            "train loss:1.3526819269996542\n",
            "train loss:1.2611858973866845\n",
            "=== epoch:272, train acc:0.55, test acc:0.4867 ===\n",
            "train loss:1.3965196131456181\n",
            "train loss:1.531327178488582\n",
            "train loss:1.320845957411167\n",
            "=== epoch:273, train acc:0.55, test acc:0.4878 ===\n",
            "train loss:1.3667455868339877\n",
            "train loss:1.4510815752611952\n",
            "train loss:1.3802813050468856\n",
            "=== epoch:274, train acc:0.5533333333333333, test acc:0.4885 ===\n",
            "train loss:1.4865149644110616\n",
            "train loss:1.2843811844761959\n",
            "train loss:1.394463189047316\n",
            "=== epoch:275, train acc:0.5466666666666666, test acc:0.4876 ===\n",
            "train loss:1.3068082113981196\n",
            "train loss:1.4121745443001386\n",
            "train loss:1.4339077641189113\n",
            "=== epoch:276, train acc:0.5466666666666666, test acc:0.4883 ===\n",
            "train loss:1.2927128925970828\n",
            "train loss:1.327677707028323\n",
            "train loss:1.3699149438862106\n",
            "=== epoch:277, train acc:0.5433333333333333, test acc:0.4867 ===\n",
            "train loss:1.3365489377441893\n",
            "train loss:1.1988221576829445\n",
            "train loss:1.4340985904646255\n",
            "=== epoch:278, train acc:0.55, test acc:0.4879 ===\n",
            "train loss:1.4082640268982123\n",
            "train loss:1.3268960997531023\n",
            "train loss:1.2066761761803986\n",
            "=== epoch:279, train acc:0.5466666666666666, test acc:0.4863 ===\n",
            "train loss:1.3806847759754168\n",
            "train loss:1.4684776385657528\n",
            "train loss:1.3188829881884272\n",
            "=== epoch:280, train acc:0.5466666666666666, test acc:0.489 ===\n",
            "train loss:1.5208439143107566\n",
            "train loss:1.6124632582685672\n",
            "train loss:1.2810801810358199\n",
            "=== epoch:281, train acc:0.5466666666666666, test acc:0.4908 ===\n",
            "train loss:1.4462049480643706\n",
            "train loss:1.2743818611809365\n",
            "train loss:1.4199575357475467\n",
            "=== epoch:282, train acc:0.55, test acc:0.4906 ===\n",
            "train loss:1.273892539981891\n",
            "train loss:1.3005025827518484\n",
            "train loss:1.3063740676988917\n",
            "=== epoch:283, train acc:0.55, test acc:0.4897 ===\n",
            "train loss:1.454705049684161\n",
            "train loss:1.3727934789991467\n",
            "train loss:1.3399095264507739\n",
            "=== epoch:284, train acc:0.55, test acc:0.4899 ===\n",
            "train loss:1.4041774823351714\n",
            "train loss:1.2270226163987656\n",
            "train loss:1.1592532694187234\n",
            "=== epoch:285, train acc:0.5466666666666666, test acc:0.4892 ===\n",
            "train loss:1.315276286199337\n",
            "train loss:1.1979890156546096\n",
            "train loss:1.2476234602570357\n",
            "=== epoch:286, train acc:0.5533333333333333, test acc:0.4918 ===\n",
            "train loss:1.3488781215311634\n",
            "train loss:1.358495055541634\n",
            "train loss:1.3622159180282012\n",
            "=== epoch:287, train acc:0.5533333333333333, test acc:0.4944 ===\n",
            "train loss:1.3590627310747798\n",
            "train loss:1.178438656324743\n",
            "train loss:1.3259132295830431\n",
            "=== epoch:288, train acc:0.56, test acc:0.4957 ===\n",
            "train loss:1.290571932259132\n",
            "train loss:1.2926502122490116\n",
            "train loss:1.3527032966067665\n",
            "=== epoch:289, train acc:0.56, test acc:0.4954 ===\n",
            "train loss:1.349259730425006\n",
            "train loss:1.2978145532807164\n",
            "train loss:1.4259056278700006\n",
            "=== epoch:290, train acc:0.5633333333333334, test acc:0.4969 ===\n",
            "train loss:1.274071222993571\n",
            "train loss:1.2171793655609482\n",
            "train loss:1.270471201038152\n",
            "=== epoch:291, train acc:0.5633333333333334, test acc:0.4975 ===\n",
            "train loss:1.3577144333558007\n",
            "train loss:1.1994517412585899\n",
            "train loss:1.279185834292058\n",
            "=== epoch:292, train acc:0.5566666666666666, test acc:0.4973 ===\n",
            "train loss:1.2721808095071436\n",
            "train loss:1.4588649151251674\n",
            "train loss:1.4037218335197332\n",
            "=== epoch:293, train acc:0.56, test acc:0.4981 ===\n",
            "train loss:1.4381536522051552\n",
            "train loss:1.3836886513140825\n",
            "train loss:1.3354579850268526\n",
            "=== epoch:294, train acc:0.56, test acc:0.4992 ===\n",
            "train loss:1.296461869583777\n",
            "train loss:1.325075482322456\n",
            "train loss:1.1495543191194135\n",
            "=== epoch:295, train acc:0.5633333333333334, test acc:0.5001 ===\n",
            "train loss:1.2936990756261355\n",
            "train loss:1.4396700234383024\n",
            "train loss:1.323915254387651\n",
            "=== epoch:296, train acc:0.57, test acc:0.5004 ===\n",
            "train loss:1.368799775267465\n",
            "train loss:1.43575424852024\n",
            "train loss:1.3964799838063329\n",
            "=== epoch:297, train acc:0.5733333333333334, test acc:0.5021 ===\n",
            "train loss:1.1936377020016604\n",
            "train loss:1.321279821331155\n",
            "train loss:1.2221284320445067\n",
            "=== epoch:298, train acc:0.58, test acc:0.5042 ===\n",
            "train loss:1.1709778100134398\n",
            "train loss:1.2337523123385814\n",
            "train loss:1.151090220389922\n",
            "=== epoch:299, train acc:0.5733333333333334, test acc:0.5035 ===\n",
            "train loss:1.2624414032202722\n",
            "train loss:1.2451416412216987\n",
            "train loss:1.3232182565189412\n",
            "=== epoch:300, train acc:0.5766666666666667, test acc:0.5037 ===\n",
            "train loss:1.3270668102721803\n",
            "train loss:1.256533089035143\n",
            "train loss:1.3780358198399305\n",
            "=== epoch:301, train acc:0.58, test acc:0.5041 ===\n",
            "train loss:1.098143270320967\n",
            "train loss:1.204144990715288\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.5026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg8LTs4rKib2"
      },
      "source": [
        "#적절한 하이퍼파라미터 값을 찾는 방법\n",
        "#1. 검증 데이터 : 하이퍼파라미터를 평가할 때는 시험 데이터를 사용하지 않아야하기 때문에, \n",
        "#훈련데이터의 일부를 하이퍼파라미터 조정용 검증데이터로 분리하는 방법 사용\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
        "\n",
        "#훈련데이터 섞기\n",
        "from common.util import shuffle_dataset\n",
        "x_train,t_train = shuffle_dataset(x_train, t_train)\n",
        "\n",
        "#20%를 검증데이터로 사용\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0]* validation_rate)\n",
        "\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2U8jsLRMqSU",
        "outputId": "a55b7f95-a780-4eac-be37-5f736131e9e3"
      },
      "source": [
        "#2. 하이퍼파라미터 최적화 : 대략적인 범위를 설정하고, 그 범위에서 하이퍼파라미터 값을 골라 샘플링후, 그 값으로 정확도를 평가하는 것을 반복\n",
        "#최적화에 오랜 시간이 걸리기 때문에 에폭을 작게하는 것이 효과적\n",
        "#0단계 : 하이퍼파라미터 값의 범위를 설정\n",
        "#1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출\n",
        "#2단계 : 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가(에폭은 작게 설정)\n",
        "#3단계 : 1~2단계를 특정 횟수 반복하여, 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "def __train(lr, weight_decay, epocs=50):\n",
        "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10, weight_decay_lambda=weight_decay)\n",
        "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
        "                      epochs=epocs, mini_batch_size=100,\n",
        "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 무작위 탐색\n",
        "optimization_trial = 100\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "for _ in range(optimization_trial):\n",
        "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "    # ================================================\n",
        "\n",
        "#실행시간이 오래 걸려서 주석처리\n",
        "#    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
        "#    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
        "#    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
        "#    results_val[key] = val_acc_list\n",
        "#    results_train[key] = train_acc_list\n",
        "\n",
        "# 그래프 그리기\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
        "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5: plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Hyper-Parameter Optimization Result ===========\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}