{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMHIH52f0wUd31e/FBfkJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lemon-Aki/DeepLearningFromScratch1/blob/main/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCkdCYjtfEvi"
      },
      "source": [
        "#오차역전파법 : 가중치 매개변수의 기울기를 효율적으로 계산, 수치 미분은 계산 시간이 오래걸림\n",
        "#계산 그래프 : 오차역전파법을 설명하는 방법 중 하나. 계산 과정을 그래프로 그린것, 노드(node)와 에지(edge)로 표현\n",
        "#에지 : 노드 사이의 직선\n",
        "#계산 그래프를 구성한다 -> 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다(순전파)\n",
        "#역전파 : 그래프에서 계산을 반대 방향(오른쪽에서 왼쪽)으로 전파\n",
        "#국소적 계산 : 자신과 직접 관계된 작은 범위의 정보만으로 결과를 출력할 수 있음\n",
        "#합성 함수 : 여러개로 구성된 함수\n",
        "#연쇄법칙 : 합성 함수의 미분은 합성 함수를 각 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dANxJ188s2D",
        "outputId": "ff76395d-6390-4b6d-a258-33dae7eef72c"
      },
      "source": [
        "# 코랩과 구글드라이드를 연동(인증 필요)\n",
        "#Transport endpoint is not connected 에러시 코랩 재연결\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/MyDrive/DeepLearningFromScratch1/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/MyDrive/DeepLearningFromScratch1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-uiG364lNpV",
        "outputId": "60c2df4c-b340-41bc-9be5-decb83d33dbc"
      },
      "source": [
        "#단순한 계층 구현\n",
        "#곱셈계층\n",
        "class MulLayer : \n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  #순전파\n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "  \n",
        "  #역전파\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "#계층 생성\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "#순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)\n",
        "\n",
        "#역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220.00000000000003\n",
            "2.2 110.00000000000001 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klVVNz0ym3h5",
        "outputId": "ff958d6a-a918-4a00-960a-e07f8133c15b"
      },
      "source": [
        "#덧셈 계층\n",
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "    return dx, dy\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "#계층 생성\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_price_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "#순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_price_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "#역전파\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price = add_price_layer.backward(dall_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "\n",
        "print(price)\n",
        "print(dapple, dapple_num, dorange, dorange_num, dtax)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "715.0000000000001\n",
            "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjlM8J4Im4wi",
        "outputId": "9fe06fc3-c880-40e8-c8d1-92a1a5a0e0f7"
      },
      "source": [
        "#Relu 계층 구현(0보다 크면 받은값을 그대로 전달, 0이하일 경우 0을 전달)\n",
        "import numpy as np\n",
        "\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    #mask : True/False로 구성된 넘파이 배열\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    print(self.mask)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx\n",
        "\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)\n",
        "\n",
        "mask = (x<= 0)\n",
        "print(mask)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsFftd3Dy86-"
      },
      "source": [
        "#Sigmoid 계층 구현(전체합이 1이 되도록 하여 비율(확률)을 구할 수 있음)\n",
        "class Sigmoid:\n",
        "  #인스턴스 변수 out에 순전파의 출력을 보관, 역전파 계산 때 그 값을 사용\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + exp(-x))\n",
        "    self.out = out\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0cm1ydJ1dsT"
      },
      "source": [
        "#Affine 계층 구현(신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서 어파인 변환이라고 함)\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis = 0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWDIpaGW6WRW"
      },
      "source": [
        "#Softmax-with-Loss 계층 : 입력값을 정규화하여 출력\n",
        "#분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정\n",
        "from common.functions import cross_entropy_error\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None    #손실\n",
        "    self.y = None     #softmax 출력값\n",
        "    self.t = None     #정답 레이블(원-핫 벡트)\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch_eJp1m9TkJ"
      },
      "source": [
        "#신경망 학습의 전체 그림\n",
        "#전체 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 함\n",
        "#학습 1단계 - 미니배치\n",
        "#훈련 데이터 중 일부를 무작위로 가져옴. 이 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
        "#학습 2단계 - 기울기 산출\n",
        "#미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 기울기는 손실 함수의 값을 가장 작게하는 방향을 제시\n",
        "#학습 3단계 - 매개변수 갱신\n",
        "#가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
        "#학습 4단계 - 1~3단계 반복\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    \n",
        "    #가중치 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    #계층 생성\n",
        "    #OrderedDict() : 순서가 있는 딕셔너리\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    #순전파\n",
        "    self.loss(x, t)\n",
        "\n",
        "    #역전파\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    #결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUzL4R4KGCln",
        "outputId": "23d9c231-1424-4ce2-df53-5f04c1ded21d"
      },
      "source": [
        "#기울기 확인 : 수치미분, 오차역전파법으로 구현한 기울기가 일치하는지 확인하는 작업\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "#각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다\n",
        "for key in grad_numerical.keys():\n",
        "  diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "  print(key + ' : ' + str(diff))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 : 5.355552638790701e-10\n",
            "b1 : 3.2651183181871877e-09\n",
            "W2 : 7.261029676085967e-09\n",
            "b2 : 1.4011373938999095e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0iP9xxEHoVT",
        "outputId": "a3b16051-419d-4f1e-831f-ebc3ce146349"
      },
      "source": [
        "#오차역전파법을 사용한 학습 구현\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #오차역전파법으로 기울기를 구함\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  #갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i % iter_per_epoch ==0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10863333333333333 0.1049\n",
            "0.90685 0.9092\n",
            "0.9265666666666666 0.9283\n",
            "0.9370833333333334 0.9354\n",
            "0.9454166666666667 0.9452\n",
            "0.9514833333333333 0.9494\n",
            "0.9563166666666667 0.9529\n",
            "0.9610333333333333 0.9572\n",
            "0.9635 0.9583\n",
            "0.96695 0.9601\n",
            "0.9690833333333333 0.9637\n",
            "0.9715333333333334 0.9658\n",
            "0.9727333333333333 0.9679\n",
            "0.9750833333333333 0.9662\n",
            "0.9761333333333333 0.9687\n",
            "0.97695 0.968\n",
            "0.9782666666666666 0.9705\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}